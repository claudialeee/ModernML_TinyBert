{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=Z474sSC6oe7A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DEfSbAA4QHas",
    "outputId": "a6536544-6bd1-462a-d946-ec6bdb8a1a2d"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Get the GPU device name.\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# # The device name should look like the following:\n",
    "# if device_name == '/device:GPU:0':\n",
    "#     print('Found GPU at: {}'.format(device_name))\n",
    "# else:\n",
    "#     raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqG7FzRVFEIv"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "oYsV4H8fCpZ-",
    "outputId": "47d8fbc6-39b6-4067-a37b-c792342c54b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def get_device():\n",
    "  # If there's a GPU available...\n",
    "  if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "  # If not...\n",
    "  else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "  return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "0NmMdkZO8R6q",
    "outputId": "81878b39-ed58-4dce-cf0a-24ef4a470dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\leedt\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\leedt\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5m6AnuFv0QXQ",
    "outputId": "dad7440c-0ec0-4d80-b80d-a6f96efa7787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\leedt\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# import os\n",
    "\n",
    "# print('Downloading dataset...')\n",
    "\n",
    "# def download_data(task):\n",
    "#     pass\n",
    "#     #data files at 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/\n",
    "#     #o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4'\n",
    "#     # file at cola.zip to cola/\n",
    "# #   task_to_data = {\n",
    "# #     'cola': ('https://nyu-mll.github.io/CoLA/cola_public_1.0.zip', './cola_public_1.0.zip', './cola_public_1.0/')\n",
    "# #   }\n",
    "  \n",
    "# #   url, download_file, unzip_file = task_to_data[task]\n",
    "  \n",
    "# #   # Download the file (if we haven't already)\n",
    "# #   if not os.path.exists(download_file):\n",
    "# #       wget.download(url, download_file)\n",
    "      \n",
    "# #   # Unzip the dataset (if we haven't already)\n",
    "# #   if not os.path.exists(unzip_file):\n",
    "# #       !unzip $download_file\n",
    "\n",
    "# download_data(\"cola\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pMtmPMkBzrvs",
    "outputId": "8923f4fa-e40c-412d-a644-f65793c3cd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "def download_data(task):\n",
    "  task_to_data = {\n",
    "    'cola': ('https://nyu-mll.github.io/CoLA/cola_public_1.1.zip', './cola_public_1.1.zip', './cola_public/')\n",
    "  }\n",
    "  \n",
    "  url, download_file, unzip_file = task_to_data[task]\n",
    "  \n",
    "  # Download the file (if we haven't already)\n",
    "  if not os.path.exists(download_file):\n",
    "      wget.download(url, download_file)\n",
    "      \n",
    "  # Unzip the dataset (if we haven't already)\n",
    "  if not os.path.exists(unzip_file):\n",
    "      !unzip $unzip_file\n",
    "\n",
    "download_data(\"cola\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "_UkeC7SG2krJ",
    "outputId": "47ae0de6-4c95-4b82-ad0f-e3fb27b8ef76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_source  label label_notes  \\\n",
       "0            gj04      1         NaN   \n",
       "1            gj04      1         NaN   \n",
       "2            gj04      1         NaN   \n",
       "3            gj04      1         NaN   \n",
       "4            gj04      1         NaN   \n",
       "\n",
       "                                            sentence  \n",
       "0  Our friends won't buy this analysis, let alone...  \n",
       "1  One more pseudo generalization and I'm giving up.  \n",
       "2   One more pseudo generalization or I'm giving up.  \n",
       "3     The more we study verbs, the crazier they get.  \n",
       "4          Day by day the facts are getting murkier.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(task, sample_data = False):\n",
    "  if task == 'cola':\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, \n",
    "                     names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "#     df = pd.read_csv(\"./cola/train.tsv\", delimiter='\\t', header=None, \n",
    "#                      names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "\n",
    "    # Report the number of sentences.\n",
    "  print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "  \n",
    "  if sample_data:\n",
    "    df = df.head(sample_data)\n",
    "  \n",
    "  return df\n",
    "\n",
    "\n",
    "df = load_data(\"cola\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.704362062916618, (8551, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].mean(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z474sSC6oe7A",
    "outputId": "7e93e7c9-9edf-473b-dbe9-eb78cdab6996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cKsH2sU0OCQA",
    "outputId": "0a95553f-5911-419f-b12f-f72b8f56a235"
   },
   "outputs": [],
   "source": [
    "# max_len = 0\n",
    "\n",
    "# # For every sentence...\n",
    "# for sent in sentences:\n",
    "\n",
    "#     # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "#     input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "#     # Update the maximum sentence length.\n",
    "#     max_len = max(max_len, len(input_ids))\n",
    "\n",
    "# print('Max sentence length: ', max_len)\n",
    "\n",
    "max_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "2bBdb3pt8LuQ",
    "outputId": "b34143b4-dac4-4240-ee92-f3694fd566f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
      "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "def get_features(task, df = None):\n",
    "  if task == 'cola':\n",
    "    return get_features_cola(df)\n",
    "    \n",
    "def get_features_cola(df):\n",
    "    # Get the lists of sentences and their labels.\n",
    "    sentences = df.sentence.values\n",
    "    labels = df.label.values\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 64,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt', \n",
    "                            truncation = True # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original: ', sentences[0])\n",
    "    print('Token IDs:', input_ids[0])\n",
    "    return input_ids, attention_masks, labels\n",
    "  \n",
    "input_ids, attention_masks, labels = get_features(\"cola\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "def get_kfold(input_ids, attention_masks, labels, task, k=3):\n",
    "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "  idx = np.arange(len(input_ids))\n",
    "  np.random.shuffle(idx)\n",
    "\n",
    "  fold_length = len(input_ids)//k\n",
    "  data = []\n",
    "\n",
    "  for i in range(k):\n",
    "    start_idx = i*fold_length\n",
    "    end_idx = (i+1)*fold_length\n",
    "    if i == k-1:\n",
    "      end_idx = len(input_ids)\n",
    "\n",
    "    validation_idx = idx[start_idx: end_idx]\n",
    "    train_idx = np.concatenate((idx[0: start_idx], idx[end_idx: len(input_ids)]))\n",
    "    validation_set = TensorDataset(*dataset[validation_idx])\n",
    "    training_set = TensorDataset(*dataset[train_idx])\n",
    "\n",
    "    data.append((training_set, validation_set))\n",
    "  return data\n",
    "  \n",
    "data = get_kfold(input_ids, attention_masks, labels, \"cola\", k=3)\n",
    "full_dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "      num_labels = 2, \n",
    "      output_attentions = False, # Whether the model returns attentions weights.\n",
    "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "batch_size = 32\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pdb\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "def check_mcc(model, prediction_dataloader):\n",
    "    predictions, true_labels = make_predictions(model, prediction_dataloader)\n",
    "    preds = np.concatenate(predictions)\n",
    "    preds = preds.argmax(1)\n",
    "\n",
    "    return matthews_corrcoef(df.label, preds)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "def make_predictions(model, prediction_dataloader):\n",
    "    # Prediction on test set\n",
    "    model.eval()\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in prediction_dataloader:\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      with torch.no_grad():\n",
    "          outputs = model(b_input_ids, token_type_ids=None, \n",
    "                          attention_mask=b_input_mask)\n",
    "      logits = outputs[0]\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      predictions.append(logits)\n",
    "      true_labels.append(label_ids)\n",
    "    model.train()\n",
    "    return predictions, true_labels\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def train_model(\n",
    "    epochs, train_dataloader, validation_dataloader, verbose = False, release = False, lr = 2e-5):\n",
    "    \n",
    "    \n",
    "  model = BertForSequenceClassification.from_pretrained(\n",
    "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "      num_labels = 2, \n",
    "      output_attentions = False, # Whether the model returns attentions weights.\n",
    "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "  )\n",
    "  mcc_arr = []\n",
    "  model = model.to(device)\n",
    "  \n",
    "  optimizer = AdamW(model.parameters(),\n",
    "                    lr = lr, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                  )\n",
    "  optimizer = optim.Adam([\n",
    "                {'params': model.bert.parameters()},\n",
    "                {'params': model.classifier.parameters(), 'lr': lr}],\n",
    "                    lr = lr/10, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                  )\n",
    "# optim.SGD([\n",
    "#                 {'params': model.base.parameters()},\n",
    "#                 {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "#             ], lr=1e-2, momentum=0.9)\n",
    "  total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "#   scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "#                                               num_warmup_steps = 0, # Default value in run_glue.py\n",
    "#                                               num_training_steps = total_steps)\n",
    "    \n",
    "#   initval = [v.cpu().detach().numpy() for v in model.parameters()]\n",
    "  # We'll store a number of quantities such as training and validation loss, \n",
    "  # validation accuracy, and timings.\n",
    "  training_stats = []\n",
    "\n",
    "  total_t0 = time.time()\n",
    "  for epoch_i in range(0, epochs):\n",
    "      if verbose:\n",
    "          print(\"\")\n",
    "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "          print('Training...')\n",
    "\n",
    "      t0 = time.time()\n",
    "      total_train_loss = 0\n",
    "      model.train()\n",
    "      for step, batch in enumerate(train_dataloader):\n",
    "          if step % 40 == 0 and not step == 0:\n",
    "              elapsed = format_time(time.time() - t0)\n",
    "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "          b_input_ids = batch[0].to(device)\n",
    "          b_input_mask = batch[1].to(device)\n",
    "          b_labels = batch[2].to(device)\n",
    "          model.zero_grad()\n",
    "          loss, logits = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask, \n",
    "                               labels=b_labels)\n",
    "          total_train_loss += loss.item()\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#           tempval = [v.cpu().detach().numpy() for v in model.parameters()]\n",
    "          optimizer.step()\n",
    "#           scheduler.step()\n",
    "#           newval = [v.cpu().detach().numpy() for v in model.parameters()]\n",
    "      avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "      training_time = format_time(time.time() - t0)\n",
    "\n",
    "      if verbose:\n",
    "          print(\"\")\n",
    "          print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "          print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "          print(\"\")\n",
    "          print(\"Running Validation...\")\n",
    "\n",
    "      t0 = time.time()\n",
    "      if release:\n",
    "        model.eval()\n",
    "        mcc = check_mcc(model, prediction_dataloader)\n",
    "        print('mcc: ', mcc)\n",
    "        mcc_arr.append(mcc)\n",
    "        continue\n",
    "      model.eval()\n",
    "\n",
    "      total_eval_accuracy = 0\n",
    "      total_eval_loss = 0\n",
    "      nb_eval_steps = 0\n",
    "\n",
    "      for batch in validation_dataloader:\n",
    "          b_input_ids = batch[0].to(device)\n",
    "          b_input_mask = batch[1].to(device)\n",
    "          b_labels = batch[2].to(device)\n",
    "          with torch.no_grad():        \n",
    "              (loss, logits) = model(b_input_ids, \n",
    "                                     token_type_ids=None, \n",
    "                                     attention_mask=b_input_mask,\n",
    "                                     labels=b_labels)\n",
    "          total_eval_loss += loss.item()\n",
    "          logits = logits.detach().cpu().numpy()\n",
    "          label_ids = b_labels.to('cpu').numpy()\n",
    "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "      # Measure how long the validation run took.\n",
    "      validation_time = format_time(time.time() - t0)\n",
    "\n",
    "      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "      print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "      # Record all statistics from this epoch.\n",
    "      training_stats.append(\n",
    "          {\n",
    "              'epoch': epoch_i + 1,\n",
    "              'Training Loss': avg_train_loss,\n",
    "              'Valid. Loss': avg_val_loss,\n",
    "              'Valid. Accur.': avg_val_accuracy,\n",
    "              'Training Time': training_time,\n",
    "              'Validation Time': validation_time\n",
    "          }\n",
    "      )\n",
    "  if release:\n",
    "    return _, _, mcc_arr, model\n",
    "  print(\"\")\n",
    "  print(\"Training complete!\")\n",
    "\n",
    "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "  return avg_val_accuracy, avg_val_loss, training_stats, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGUqOCtgqGhP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    179.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    179.    Elapsed: 0:00:21.\n",
      "  Batch   120  of    179.    Elapsed: 0:00:32.\n",
      "  Batch   160  of    179.    Elapsed: 0:00:43.\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 0:00:08\n",
      "  Batch    40  of    179.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    179.    Elapsed: 0:00:21.\n",
      "  Batch   120  of    179.    Elapsed: 0:00:32.\n",
      "  Batch   160  of    179.    Elapsed: 0:00:43.\n",
      "  Accuracy: 0.82\n",
      "  Validation Loss: 0.48\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:51 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    179.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    179.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    179.    Elapsed: 0:00:32.\n",
      "  Batch   160  of    179.    Elapsed: 0:00:43.\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 0:00:08\n",
      "  Batch    40  of    179.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    179.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    179.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    179.    Elapsed: 0:00:44.\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.48\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:52 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    179.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    179.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    179.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    179.    Elapsed: 0:00:44.\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 0:00:08\n",
      "  Batch    40  of    179.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    179.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    179.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    179.    Elapsed: 0:00:44.\n",
      "  Accuracy: 0.82\n",
      "  Validation Loss: 0.46\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:53 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "avg_val_accuracies, avg_val_losses = [], []\n",
    "for train_dataset, val_dataset in data:\n",
    "  train_dataloader = DataLoader(\n",
    "              train_dataset,  # The training samples.\n",
    "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "              batch_size = batch_size # Trains with this batch size.\n",
    "          )\n",
    "  validation_dataloader = DataLoader(\n",
    "              val_dataset, # The validation samples.\n",
    "              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "              batch_size = batch_size # Evaluate with this batch size.\n",
    "          )\n",
    "\n",
    "\n",
    "  epochs = 2\n",
    "  avg_val_accuracy, avg_val_loss, training_stats, model = train_model(\n",
    "      epochs, train_dataloader, validation_dataloader)\n",
    "  avg_val_accuracies.append(avg_val_accuracy)\n",
    "  avg_val_losses.append(avg_val_loss)\n",
    "  val_loss, val_acc = [sum(arr) / len(arr) for arr in [avg_val_losses, avg_val_accuracies]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "6O_NbXFGMukX",
    "outputId": "6686e966-46a5-4445-f899-d1f4f79a2927"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0:00:49</td>\n",
       "      <td>0:00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0:00:49</td>\n",
       "      <td>0:00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.49         0.45           0.81       0:00:49         0:00:08\n",
       "2               0.32         0.46           0.82       0:00:49         0:00:08"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "68xreA9JAmG5",
    "outputId": "265c6738-9f28-4817-a350-a63a1ac01934"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "mAN0LZBOOPVh",
    "outputId": "e97be795-f802-4c2e-8f2b-d2ac5cd2eead"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16lctEOyNFik"
   },
   "source": [
    "## 5.2. Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhR99IISNMg9"
   },
   "source": [
    "\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    268.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    268.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    268.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    268.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    268.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    268.    Elapsed: 0:01:06.\n",
      "mcc:  0.06513891332748649\n"
     ]
    }
   ],
   "source": [
    "full_dataloader = DataLoader(\n",
    "          full_dataset,  # The training samples.\n",
    "          sampler = RandomSampler(full_dataset), # Select batches randomly\n",
    "          batch_size = batch_size # Trains with this batch size.\n",
    "      )\n",
    "\n",
    "epochs = 6\n",
    "_, _, mcc_arr, model = train_model(\n",
    "  epochs, full_dataloader, None, release = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "6ulTWaOr8QNY",
    "outputId": "a5517081-2e05-4244-c8df-77a9558ff75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-tjHkR7lc1I"
   },
   "source": [
    "Let's check out the file sizes, out of curiosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "mqMzI3VTCZo5",
    "outputId": "6df0b283-6458-4d95-8455-2e7537193d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 428015K\n",
      "-rw-r--r-- 1 leedt 197612      1K Sep 21 19:38 config.json\n",
      "-rw-r--r-- 1 leedt 197612 427754K Sep 21 19:38 pytorch_model.bin\n",
      "-rw-r--r-- 1 leedt 197612      1K Sep 21 19:38 special_tokens_map.json\n",
      "-rw-r--r-- 1 leedt 197612      1K Sep 21 19:38 tokenizer_config.json\n",
      "-rw-r--r-- 1 leedt 197612    256K Sep 21 19:38 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=K ./model_save/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr_bt2rFlgDn"
   },
   "source": [
    "The largest file is the model weights, at around 418 megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-WUFUIQ8Cu8D",
    "outputId": "70780762-7790-474f-e5c2-304a066945ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leedt 197612 418M Sep 21 19:38 ./model_save/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M ./model_save/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzGKvOFAll_e"
   },
   "source": [
    "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trr-A-POC18_"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to this Notebook instance.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxlZsafTC-V5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot create directory './drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Copy the model files to a directory in your Google Drive.\n",
    "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0vstijw85SZ"
   },
   "source": [
    "The following functions will load the model back from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskPzUM084zL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-decb3e924a99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load a trained model and vocabulary that you have fine-tuned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Copy the model to the GPU.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_class' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIWouvDrGVAi"
   },
   "source": [
    "## A.2. Weight Decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f123ZAlF1OyW"
   },
   "source": [
    "The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix.\n",
    "\n",
    "This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "# This code is taken from:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
    "\n",
    "# Don't apply weight decay to any parameters whose names include these tokens.\n",
    "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# Separate the `weight` parameters from the `bias` parameters. \n",
    "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
    "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
    "optimizer_grouped_parameters = [\n",
    "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.1},\n",
    "    \n",
    "    # Filter for parameters which *do* include those.\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
    "# the names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKzLS9ohzGVu"
   },
   "source": [
    "# Revision History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZqpiHEnGqYR"
   },
   "source": [
    "**Version 3** - *Mar 18th, 2020* - (current)\n",
    "* Simplified the tokenization and input formatting (for both training and test) by leveraging the `tokenizer.encode_plus` function. \n",
    "`encode_plus` handles padding *and* creates the attention masks for us.\n",
    "* Improved explanation of attention masks.\n",
    "* Switched to using `torch.utils.data.random_split` for creating the training-validation split.\n",
    "* Added a summary table of the training statistics (validation loss, time per epoch, etc.).\n",
    "* Added validation loss to the learning curve plot, so we can see if we're overfitting. \n",
    "    * Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing this!\n",
    "* Displayed the per-batch MCC as a bar plot.\n",
    "\n",
    "**Version 2** - *Dec 20th, 2019* - [link](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP)\n",
    "* huggingface renamed their library to `transformers`. \n",
    "* Updated the notebook to use the `transformers` library.\n",
    "\n",
    "**Version 1** - *July 22nd, 2019*\n",
    "* Initial version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FL_NnDGxRpEI"
   },
   "source": [
    "## Further Work\n",
    "\n",
    "* It might make more sense to use the MCC score for “validation accuracy”, but I’ve left it out so as not to have to explain it earlier in the Notebook.\n",
    "* Seeding -- I’m not convinced that setting the seed values at the beginning of the training loop is actually creating reproducible results…\n",
    "* The MCC score seems to vary substantially across different runs. It would be interesting to run this example a number of times and show the variance.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
