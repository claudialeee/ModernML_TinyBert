{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils import CUDA_ENABLED, to_cuda, build_sentence_list\n",
    "from data import CustomDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "#from pretrained_model import *\n",
    "from transformers import BertModel, BertTokenizer, BertForMaskedLM\n",
    "from transformer import Transformer\n",
    "import math\n",
    "#from transformer import *\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "#from torch.nn.functional import leaky_relu\n",
    "import torch.nn.functional as F\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is this? Why do we even use glue?  We will use this later but it's not in use yet\n",
    "\n",
    "# import download_glue_data\n",
    "# if not os.path.isdir('glue_data'):\n",
    "#     download_glue_data.main('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['The set of those translates partitions the circle into a countable collection of disjoint sets, which are all pairwise congruent.']\n"
    }
   ],
   "source": [
    "#Wikipedia sample data\n",
    "dataset = CustomDataset('wikisample100000.txt')\n",
    "dataloader = DataLoader(dataset, batch_size = 1, num_workers = 0, shuffle=True)\n",
    "for x in dataloader: #Test that our dataloader worked fine\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "pretrained = to_cuda(BertForMaskedLM.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True))\n",
    "#PretrainedModel() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): #Keep this, removed dropout cause didn't think he needed it\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "VOCAB_SIZE = vocab_size = tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TinyBert(nn.Module):\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, emb_size=144, nhead = 12, num_encoder_layers = 6, teacher_size=768):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.model = Transformer(\n",
    "            d_model = emb_size, nhead = nhead, num_encoder_layers = num_encoder_layers, \n",
    "            dim_feedforward = emb_size, dropout = .1, activation = 'lrelu')\n",
    "        self.embedder = nn.Embedding(vocab_size, emb_size)\n",
    "        self.PE = PositionalEncoding(emb_size)\n",
    "        self.teacher_size = teacher_size\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(emb_size, teacher_size) for _ in range(num_encoder_layers + 1)])\n",
    "        self.linear_output = nn.Linear(emb_size, vocab_size)\n",
    "    def forward(self, src, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(src, dtype = float)\n",
    "        #reshaping cus trf module is stupid\n",
    "        self.mask = mask\n",
    "        self.emb_raw = emb_raw = self.embedder(src)\n",
    "        self.emb = emb = self.PE(emb_raw)\n",
    "        self.emb_transposed = emb_transposed = torch.transpose(emb, 1, 0)\n",
    "        self.hidden, self.attn = hidden, attn = self.model(emb_transposed, src_key_padding_mask=mask)\n",
    "        self.emb_hidden = [emb_transposed] + hidden\n",
    "        emb_and_hidden = [torch.transpose(l, 1, 0) for l in self.emb_hidden]\n",
    "        self.projections = projections = [l(embedding) for l, embedding in zip(self.linear_layers, emb_and_hidden)]\n",
    "        self.output_logits = output_logits = torch.transpose(self.linear_output(hidden[-1]), 1, 0)\n",
    "        self.output_probs = output_probs = F.softmax(output_logits, -1)\n",
    "        return output_probs, output_logits, projections, emb_and_hidden, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Distiller(nn.Module): #Bert Distiller we will use to train tiny bert\n",
    "    def __init__(self, num_encoder_layers=7):\n",
    "\n",
    "        super().__init__()\n",
    "        self.pretrained_model = to_cuda(BertForMaskedLM.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True))\n",
    "        self.tinybert = TinyBert(num_encoder_layers=num_encoder_layers)\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        # assuming 13 layers\n",
    "        self.step = int((13-1)/(self.num_encoder_layers-1))\n",
    "#         self.tinybert = Transformer(d_model = 100, nhead = 2, num_encoder_layers = 3, \n",
    "#                    dim_feedforward = 100, dropout = .1, activation = 'lrelu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.y = []\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward(self, text):\n",
    "        if isinstance(text[0], list):\n",
    "            return self.forward_sentence(text)\n",
    "        elif isinstance(text[0], str):\n",
    "            return self.forward_maskLM(text)\n",
    "        else:\n",
    "            raise ValueError('Invalid Text: ' + text + type(text[0]))\n",
    "            \n",
    "    def preprocess_LM(self, text):\n",
    "        self.y = []\n",
    "        sentences = [build_sentence_list(\n",
    "            'CLS', [self.tokenizer.tokenize(line)]) for line in text]\n",
    "        \n",
    "        lengths = [len(sentence) - 2 for sentence in sentences]\n",
    "        mask_idxes = [np.random.choice(length, size=math.ceil(length/7), replace=False) for length in lengths]\n",
    "        \n",
    "        masks = [np.ones(length + 2) for length in lengths]\n",
    "        for mask_idxes, mask, sentence in zip(mask_idxes, masks, sentences):\n",
    "            self.y.append([])\n",
    "            for mask_idx in mask_idxes:\n",
    "                mask[mask_idx + 1] = 0\n",
    "                self.y[-1].append(sentence[mask_idx + 1])\n",
    "                sentence[mask_idx + 1] = '[MASK]'\n",
    "        self.attention_mask = attention_mask = to_cuda(torch.tensor(pad_sequences(masks, padding='post')))\n",
    "        self.tokenized_text = tokenized_text = to_cuda(torch.tensor(pad_sequences([\n",
    "            self.tokenizer.convert_tokens_to_ids(sentence) for sentence in sentences]).tolist()))\n",
    "        return tokenized_text, attention_mask\n",
    "\n",
    "    def forward_maskLM(self, text):\n",
    "        tokenized_text, attention_mask = self.preprocess_LM(text)\n",
    "        self.pretrained_loss, self.pretrained_output, self.pretrained_hidden, self.pretrained_attn = \\\n",
    "            pretrained_loss, pretrained_output, pretrained_hidden, pretrained_attn = self.pretrained_model(\n",
    "            tokenized_text = tokenized_text, attention_mask = attention_mask)\n",
    "        self.tb_output, self.tb_logits, self.tb_projection, self.tb_hidden, self.tb_attn = \\\n",
    "            tb_output, tb_logits, tb_projection, tb_hidden, tb_attn = \\\n",
    "            self.tinybert(tokenized_text, mask=attention_mask)\n",
    "        # self.tb_out_masked = tb_out_masked = tb_out * attention_mask.transpose(1, 0).unsqueeze(-1)\n",
    "        pretrained_hidden = pretrained_hidden[::self.step]\n",
    "        pretrained_attn = pretrained_attn[::self.step]\n",
    "        return (tokenized_text, attention_mask,\n",
    "                pretrained_loss, pretrained_output, pretrained_hidden, pretrained_attn, \n",
    "                tb_output, tb_logits, tb_projection, tb_hidden, tb_attn)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TinyBert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "mdl = Bert_Distiller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put these in a model somewhere make them self. \n",
    "#Self.mseloss = nn.mseloss\n",
    "def loss_hidden(tb_projection, pretrained_hidden):\n",
    "    lossfcn = nn.MSELoss()\n",
    "    return sum([lossfcn(t, p) for t, p in zip(tb_projection, pretrained_hidden)])\n",
    "\n",
    "def loss_attn(tb_attn, pretrained_attn):\n",
    "    lossfcn = nn.MSELoss()\n",
    "    return sum([lossfcn(t, p) for t, p in zip(tb_attn, pretrained_attn)])\n",
    "    \n",
    "def loss_pred(pt_output, tb_logits):\n",
    "    m = nn.LogSoftmax()\n",
    "    return -pt_output * m(tb_logits)\n",
    "    \n",
    "def loss(pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn):\n",
    "    L_hid = loss_hidden(tb_projection, pt_hidden)\n",
    "    L_attn = loss_attn(tb_attn, pt_attn)\n",
    "    L_pred = loss_pred(pt_output, tb_logits)\n",
    "    return L_hid + L_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(mdl, text, i): #Plug in model?  #Maybe keep this outside the model\n",
    "    mdl.zero_grad()\n",
    "    tok_txt, msk, pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn = mdl(text)\n",
    "    loss_val = loss(pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn)\n",
    "    loss_val.backward()\n",
    "    mdl.optimizer.step()\n",
    "    if i % 3 == 0:\n",
    "        print(loss_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Unexpected keyword arguments: ['tokenized_text'].",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-a4b61e2104cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mitr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtok_txt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_projection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_projection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_attn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mloss_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-51edbc3d3862>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_maskLM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wtf is this text?'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-51edbc3d3862>\u001b[0m in \u001b[0;36mforward_maskLM\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_attn\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             pretrained_loss, pretrained_output, pretrained_hidden, pretrained_attn = self.pretrained_model(\n\u001b[1;32m---> 48\u001b[1;33m             tokenized_text = tokenized_text, attention_mask = attention_mask)\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb_projection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb_attn\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mtb_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_projection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_attn\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, **kwargs)\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"masked_lm_labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;34m\"lm_labels\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Use `BertWithLMHead` for autoregressive language modeling task.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         outputs = self.bert(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Unexpected keyword arguments: ['tokenized_text']."
     ]
    }
   ],
   "source": [
    "##Run the model a couple times and view loss every three times\n",
    "\n",
    "itr = 0\n",
    "for text in dataloader:\n",
    "    itr += 1\n",
    "    mdl.zero_grad()\n",
    "    tok_txt, msk, pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn = mdl(text)\n",
    "    loss_val = loss(pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn)\n",
    "    loss_val.backward()\n",
    "    mdl.optimizer.step()\n",
    "    if itr % 3 == 0:\n",
    "        print(loss_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1600130345295"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}