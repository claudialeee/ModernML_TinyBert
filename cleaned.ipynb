{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from data import *\n",
    "from utils import *\n",
    "from pretrained_model import *\n",
    "from transformer import *\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.functional import leaky_relu\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import download_glue_data\n",
    "if not os.path.isdir('glue_data'):\n",
    "    download_glue_data.main('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('sample100000.txt')\n",
    "dataloader = DataLoader(dataset, batch_size = 2, num_workers = 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf =  Transformer(d_model = 100, nhead = 2, num_encoder_layers = 3, \n",
    "                   dim_feedforward = 100, dropout = .1, activation = 'lrelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trf_output, trf_attn = trf(torch.rand((10, 32, 100)), src_key_padding_mask=torch.ones((32, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[l.shape for l in trf_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[l.shape for l in trf_attn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(trf_attn[0].sum(-1).detach().numpy(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = PretrainedModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(torch.rand((10, 32, 100)), 1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedder(5, 4)\n",
    "# PE(emb(torch.tensor([[1, 2, 3]]))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "VOCAB_SIZE = vocab_size = tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1 = W1 * x\n",
    "# h2 = W2 * h1 + h1\n",
    "# h3 = W3 * h2 + h2\n",
    "# y = W4 * h3 + h3\n",
    "\n",
    "# d(L)/w1 = d(l/y)*W4*W3*W2*x + <> +\n",
    "\n",
    "# d(L/W1) = d(l/y) * (1 + W4) * (1 + W3) * (1 + W2) * x\n",
    "\n",
    "# d(l/y) * x * (1 + W4 + W3 + W2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBert(nn.Module):\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, emb_size=144, nhead = 12, num_encoder_layers = 6, teacher_size=768):\n",
    "        super(TinyBert, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.model = Transformer(\n",
    "            d_model = emb_size, nhead = nhead, num_encoder_layers = num_encoder_layers, \n",
    "            dim_feedforward = emb_size, dropout = .1, activation = 'lrelu')\n",
    "        self.embedder = Embedder(vocab_size, emb_size)\n",
    "        self.PE = PositionalEncoding(emb_size)\n",
    "        self.teacher_size = teacher_size\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(emb_size, teacher_size) for _ in range(num_encoder_layers + 1)])\n",
    "        self.linear_output = nn.Linear(emb_size, vocab_size)\n",
    "    def forward(self, src, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(src, dtype = float)\n",
    "        #reshaping cus trf module is stupid\n",
    "        self.mask = mask\n",
    "        self.emb_raw = emb_raw = self.embedder(src)\n",
    "        self.emb = emb = self.PE(emb_raw)\n",
    "        self.emb_transposed = emb_transposed = torch.transpose(emb, 1, 0)\n",
    "        self.hidden, self.attn = hidden, attn = self.model(emb_transposed, src_key_padding_mask=mask)\n",
    "        self.emb_hidden = [emb_transposed] + hidden\n",
    "        emb_and_hidden = [torch.transpose(l, 1, 0) for l in self.emb_hidden]\n",
    "        self.projections = projections = [l(embedding) for l, embedding in zip(self.linear_layers, emb_and_hidden)]\n",
    "        self.output_logits = output_logits = torch.transpose(self.linear_output(hidden[-1]), 1, 0)\n",
    "        self.output_probs = output_probs = F.softmax(output_logits, -1)\n",
    "        return output_probs, output_logits, projections, emb_and_hidden, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TinyBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_encoder_layers = 7):\n",
    "        super(Model, self).__init__()\n",
    "        self.pretrained_model = PretrainedModel()\n",
    "        self.tinybert = TinyBert(num_encoder_layers=num_encoder_layers)\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        # assuming 13 layers\n",
    "        self.step = int((13-1)/(self.num_encoder_layers-1))\n",
    "#         self.tinybert = Transformer(d_model = 100, nhead = 2, num_encoder_layers = 3, \n",
    "#                    dim_feedforward = 100, dropout = .1, activation = 'lrelu')\n",
    "        self.tokenizer = self.pretrained_model.tokenizer\n",
    "        self.y = []\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward(self, text):\n",
    "        if isinstance(text[0], list):\n",
    "            return self.forward_sentence(text)\n",
    "        elif isinstance(text[0], str):\n",
    "            return self.forward_maskLM(text)\n",
    "        else:\n",
    "            raise ValueError('wtf is this text?' + text + type(text[0]))\n",
    "            \n",
    "    def preprocess_LM(self, text):\n",
    "        self.y = []\n",
    "        sentences = [build_sentence_list(\n",
    "            'CLS', [self.tokenizer.tokenize(line)]) for line in text]\n",
    "        \n",
    "        lengths = [len(sentence) - 2 for sentence in sentences]\n",
    "        mask_idxes = [np.random.choice(length, size=math.ceil(length/7), replace=False) for length in lengths]\n",
    "        \n",
    "        masks = [np.ones(length + 2) for length in lengths]\n",
    "        for mask_idxes, mask, sentence in zip(mask_idxes, masks, sentences):\n",
    "            self.y.append([])\n",
    "            for mask_idx in mask_idxes:\n",
    "                mask[mask_idx + 1] = 0\n",
    "                self.y[-1].append(sentence[mask_idx + 1])\n",
    "                sentence[mask_idx + 1] = '[MASK]'\n",
    "        self.attention_mask = attention_mask = to_cuda(torch.tensor(pad_sequences(masks, padding='post')))\n",
    "        self.tokenized_text = tokenized_text = to_cuda(torch.tensor(pad_sequences([\n",
    "            self.tokenizer.convert_tokens_to_ids(sentence) for sentence in sentences]).tolist()))\n",
    "        return tokenized_text, attention_mask\n",
    "\n",
    "    def forward_maskLM(self, text):\n",
    "        tokenized_text, attention_mask = self.preprocess_LM(text)\n",
    "        self.pretrained_loss, self.pretrained_output, self.pretrained_hidden, self.pretrained_attn = \\\n",
    "            pretrained_loss, pretrained_output, pretrained_hidden, pretrained_attn = self.pretrained_model(\n",
    "            tokenized_text = tokenized_text, attention_mask = attention_mask)\n",
    "        self.tb_output, self.tb_logits, self.tb_projection, self.tb_hidden, self.tb_attn = \\\n",
    "            tb_output, tb_logits, tb_projection, tb_hidden, tb_attn = \\\n",
    "            self.tinybert(tokenized_text, mask=attention_mask)\n",
    "        # self.tb_out_masked = tb_out_masked = tb_out * attention_mask.transpose(1, 0).unsqueeze(-1)\n",
    "        pretrained_hidden = pretrained_hidden[::self.step]\n",
    "        pretrained_attn = pretrained_attn[::self.step]\n",
    "        return (tokenized_text, attention_mask,\n",
    "                pretrained_loss, pretrained_output, pretrained_hidden, pretrained_attn, \n",
    "                tb_output, tb_logits, tb_projection, tb_hidden, tb_attn)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hidden(tb_projection, pretrained_hidden):\n",
    "    lossfcn = nn.MSELoss()\n",
    "    return sum([lossfcn(t, p) for t, p in zip(tb_projection, pretrained_hidden)])\n",
    "\n",
    "def loss_attn(tb_attn, pretrained_attn):\n",
    "    lossfcn = nn.MSELoss()\n",
    "    return sum([lossfcn(t, p) for t, p in zip(tb_attn, pretrained_attn)])\n",
    "    \n",
    "def loss_pred(pt_output, tb_logits):\n",
    "    m = nn.LogSoftmax()\n",
    "    return -pt_output * m(tb_logits)\n",
    "    \n",
    "def loss(pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn):\n",
    "    L_hid = loss_hidden(tb_projection, pt_hidden)\n",
    "    L_attn = loss_attn(tb_attn, pt_attn)\n",
    "    L_pred = loss_pred(pt_output, tb_logits)\n",
    "    return L_hid + L_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(mdl, text, i):\n",
    "    mdl.zero_grad()\n",
    "    tok_txt, msk, pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn = mdl(text)\n",
    "    loss_val = loss(pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn)\n",
    "    loss_val.backward()\n",
    "    mdl.optimizer.step()\n",
    "    if i % 3 == 0:\n",
    "        print(loss_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 0\n",
    "for text in dataloader:\n",
    "#     print(len(text), text)\n",
    "    itr += 1\n",
    "    mdl.zero_grad()\n",
    "    tok_txt, msk, pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn = mdl(text)\n",
    "    loss_val = loss(pt_loss, pt_output, pt_hidden, pt_attn, tb_output, tb_logits, tb_projection, tb_hidden, tb_attn)\n",
    "    loss_val.backward()\n",
    "    mdl.optimizer.step()\n",
    "    if itr % 3 == 0:\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfcn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfcn(tb_attn[-1], pt_attn[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb_attn[0].shape, pt_attn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    step(mdl, text, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.grad for p in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.tinybert.model.encoder.layers[0].linear1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.shape for t in tb_projection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, attention_mask, tb_projection, tb_out, tb_attn, pretrained_hidden, pretrained_attn = mdl(['hi there how are you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.model.embeddings(tokenized_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pretrained_attn), [h.shape for h in pretrained_attn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pretrained_hidden), [h.shape for h in pretrained_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True,)\n",
    "input_ids = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(input_ids, labels=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = PretrainedModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BertForMaskedLM.from_pretrained(\n",
    "            'bert-base-uncased', return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = b(torch.tensor([[10, 20, 30]]))\n",
    "len(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ret[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ret[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertForPreTraining, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.BertForPreTrainingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc',\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True)\n",
    "\n",
    "inputs = tokenizer([\"hi there.[SEP] How are you?\"], return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7632, 2045, 1012,  102, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer.modeling'; 'transformer' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-ac7c9f407915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTinyBertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformer.modeling'; 'transformer' is not a package"
     ]
    }
   ],
   "source": [
    "from transformer.modeling import TinyBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is pood on three times and then thrown out the window\"\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding=True)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "# the linear classifier still needs to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce6f4d5190490fb1d3be85f478f9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defd8cbd8f054c69b902688b36ffd037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForMultipleChoice: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMultipleChoice\n",
    "import tensorflow as tf\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForMultipleChoice.from_pretrained('bert-base-cased')\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is eaten while held in the hand.\"\n",
    "encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='tf', padding=True)\n",
    "inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}\n",
    "outputs = model(inputs)  # batch size is 1\n",
    "# the linear classifier still needs to be trained\n",
    "logits = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6946, grad_fn=<NllLossBackward>),\n",
       " tensor([[-0.5508, -0.5479]], grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8309, grad_fn=<NllLossBackward>),\n",
       " tensor([[ 1.0453, -0.6110]], grad_fn=<AddmmBackward>),\n",
       " (tensor([[[ 0.4920,  0.1265, -0.2230,  ...,  0.0460,  0.0441, -0.1251],\n",
       "           [-1.2375, -0.2857, -0.5368,  ..., -0.3587, -0.5758,  0.3240],\n",
       "           [ 0.3236,  0.1171,  0.4229,  ..., -0.5042,  0.9461,  0.6759],\n",
       "           ...,\n",
       "           [ 0.7489, -0.9659, -0.0346,  ...,  0.6375, -0.7057, -0.6425],\n",
       "           [-0.2366, -0.0164, -0.2193,  ..., -0.2918,  0.6380, -0.6107],\n",
       "           [ 0.0089,  0.0821,  0.3388,  ...,  0.5492, -0.6099,  0.4879]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.3712, -0.0675, -0.1078,  ...,  0.0038,  0.0427, -0.1037],\n",
       "           [-1.6868, -0.7524, -0.8203,  ..., -0.5361, -0.8800,  0.7087],\n",
       "           [ 0.4765, -0.0767,  0.5656,  ..., -1.1559,  1.7221,  0.7663],\n",
       "           ...,\n",
       "           [ 1.1163, -1.3334,  0.1978,  ...,  0.2072, -0.9557, -1.1962],\n",
       "           [-0.2739,  0.2377, -0.4555,  ..., -0.8546,  0.6964, -0.9835],\n",
       "           [ 0.4030, -0.4792,  0.5058,  ...,  0.5861, -0.5341,  0.4136]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.5508, -0.0346, -0.2460,  ...,  0.0223,  0.2488, -0.1482],\n",
       "           [-1.0305, -1.0705, -0.7284,  ..., -0.9771, -0.5996,  0.5509],\n",
       "           [ 0.5274, -0.0606,  0.4625,  ..., -1.4629,  1.6330,  0.9123],\n",
       "           ...,\n",
       "           [ 1.0379, -1.6238, -0.3407,  ...,  0.2941, -0.6989, -0.6974],\n",
       "           [ 0.3729,  0.2144, -0.9737,  ..., -0.7607,  0.3322, -0.5201],\n",
       "           [ 0.1115, -0.2444,  0.1341,  ...,  0.2890, -0.1568,  0.0749]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.6933,  0.7695, -0.2934,  ..., -0.0530,  0.0747,  0.4370],\n",
       "           [-1.1295, -0.9891, -0.5720,  ..., -0.6988, -0.5094, -0.0302],\n",
       "           [-0.0415, -0.3193,  0.5841,  ..., -1.6943,  1.3018,  0.5846],\n",
       "           ...,\n",
       "           [ 1.3316, -1.7203, -0.5013,  ...,  0.4829, -1.2316, -0.7623],\n",
       "           [ 0.2922,  0.3343, -0.9177,  ..., -0.7089, -0.0243, -0.2869],\n",
       "           [ 0.0551, -0.0561,  0.0579,  ...,  0.0472, -0.0383,  0.0753]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.0036,  0.6612, -0.3816,  ..., -0.3133,  0.5238, -0.2012],\n",
       "           [-0.5867, -1.2786, -0.4121,  ..., -1.2335, -0.0869, -0.2517],\n",
       "           [ 0.1021,  0.1520,  1.0421,  ..., -1.5774,  1.2671,  0.3860],\n",
       "           ...,\n",
       "           [ 1.4395, -1.5747, -0.3909,  ...,  0.2245, -0.6859, -1.1999],\n",
       "           [ 0.8705, -0.5261, -0.7089,  ..., -0.9557,  0.0509, -0.2666],\n",
       "           [ 0.0296,  0.0068,  0.0258,  ...,  0.0220, -0.0131, -0.0636]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.9462, -0.0840,  0.1275,  ...,  0.4513,  0.2853, -0.5538],\n",
       "           [-0.3709, -1.7467, -0.3964,  ..., -0.7986, -0.5041, -0.3338],\n",
       "           [ 0.3242, -0.1992,  0.8928,  ..., -1.2058,  1.0830,  0.1109],\n",
       "           ...,\n",
       "           [ 1.6132, -1.9134, -0.4309,  ...,  0.3667, -0.5019, -1.5270],\n",
       "           [ 1.6021, -0.7006, -0.0575,  ..., -0.6726, -0.6966, -0.3397],\n",
       "           [ 0.0024, -0.0560,  0.0663,  ..., -0.0315, -0.0399, -0.0762]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.4268, -0.0487,  0.0973,  ...,  0.6523, -0.1193, -1.0848],\n",
       "           [-0.8058, -1.2206, -0.3364,  ..., -0.6166, -0.7095, -0.6997],\n",
       "           [-0.1046, -0.7025,  0.5500,  ..., -1.1404,  0.5292,  0.0062],\n",
       "           ...,\n",
       "           [ 0.7958, -1.3370, -0.0114,  ...,  0.1028, -0.4482, -1.0658],\n",
       "           [ 1.1094, -0.4503, -0.0031,  ..., -0.2679, -0.8398,  0.1687],\n",
       "           [-0.0339, -0.0147,  0.0759,  ..., -0.0625, -0.0042, -0.1315]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.3280,  0.1744,  0.2151,  ...,  0.0960, -0.0941, -0.9686],\n",
       "           [-0.4490, -0.9061,  0.0176,  ..., -0.5871, -0.5380, -0.2122],\n",
       "           [-0.1441, -0.6696,  0.3611,  ..., -1.3825,  0.5326,  0.0746],\n",
       "           ...,\n",
       "           [ 0.7716, -0.8027,  0.5637,  ...,  0.3616, -0.2227, -0.7583],\n",
       "           [ 0.8453, -0.5588,  0.2368,  ..., -0.3725, -1.0018,  0.2061],\n",
       "           [-0.0682,  0.0282,  0.0663,  ..., -0.0308, -0.0603, -0.1306]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.3440,  0.5151,  0.3293,  ...,  0.0765, -0.0199, -0.4596],\n",
       "           [-0.5241, -0.5935,  0.0991,  ..., -0.6343, -0.7564, -0.1603],\n",
       "           [-0.0350, -0.5723,  0.0342,  ..., -1.0998,  0.1284,  0.2460],\n",
       "           ...,\n",
       "           [ 0.3716, -0.9438,  0.2858,  ..., -0.2147, -0.1529, -0.4988],\n",
       "           [ 0.2556, -0.7728,  0.4423,  ..., -0.1919, -0.9526, -0.2286],\n",
       "           [-0.0489,  0.0705,  0.0447,  ...,  0.0048,  0.0336, -0.0968]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.5475,  0.7496,  0.2389,  ...,  0.2911,  0.6150, -0.1225],\n",
       "           [-0.3983, -0.4197,  0.0476,  ..., -0.7321, -0.8222,  0.1719],\n",
       "           [ 0.2269, -0.7931, -0.1543,  ..., -0.9274, -0.0579,  0.1877],\n",
       "           ...,\n",
       "           [ 0.7587, -0.7249,  0.2572,  ...,  0.1944, -0.5366, -0.4670],\n",
       "           [ 0.1135, -0.8496,  0.4269,  ..., -0.3480, -1.4013,  0.0489],\n",
       "           [-0.0421,  0.0959,  0.0248,  ...,  0.0259,  0.0027, -0.1135]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.4947,  0.5579,  0.0985,  ...,  0.5422,  0.8066, -0.5900],\n",
       "           [-0.2772, -0.5739,  0.4447,  ..., -0.7153, -0.7946, -0.1736],\n",
       "           [ 0.1044, -0.8090,  0.1153,  ..., -0.5543,  0.1925, -0.0978],\n",
       "           ...,\n",
       "           [ 0.3774, -0.9060, -0.0489,  ...,  0.3348, -0.2801, -0.6237],\n",
       "           [ 0.4238, -0.9133,  0.4350,  ..., -0.2385, -1.0993, -0.0046],\n",
       "           [-0.0660,  0.0538,  0.0468,  ..., -0.0106, -0.0245, -0.0835]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.3114,  0.2294,  0.4750,  ...,  0.3100,  0.9192, -0.3846],\n",
       "           [ 0.0251, -0.7256,  0.8490,  ..., -0.8731, -0.8450, -0.5109],\n",
       "           [ 0.1394, -0.7262,  0.3272,  ..., -0.6435,  0.1077, -0.1213],\n",
       "           ...,\n",
       "           [ 0.3885, -0.8962,  0.3330,  ...,  0.3386, -0.1997, -0.3561],\n",
       "           [ 0.6828, -1.1805,  0.6162,  ..., -0.1768, -0.7516, -0.1817],\n",
       "           [-0.0744,  0.0561,  0.0227,  ..., -0.0039,  0.0083, -0.0594]]],\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.0339,  0.0704,  0.2267,  ...,  0.2059,  0.2182, -0.0953],\n",
       "           [ 0.2655, -0.5476,  0.4867,  ..., -0.2173, -0.3306, -0.0407],\n",
       "           [ 0.0666, -0.2378,  0.0795,  ..., -0.2677,  0.0876, -0.0258],\n",
       "           ...,\n",
       "           [-0.0612, -0.4000,  0.1737,  ...,  0.3013, -0.1712, -0.1196],\n",
       "           [ 0.1843, -0.4596,  0.1926,  ...,  0.0406, -0.3841, -0.1563],\n",
       "           [ 0.8185, -0.5925,  0.1601,  ..., -0.1330, -1.1431, -0.1917]]],\n",
       "         grad_fn=<NativeLayerNormBackward>)),\n",
       " (tensor([[[[4.3425e-01, 6.9586e-03, 4.5239e-03,  ..., 9.6974e-03,\n",
       "             1.5085e-02, 2.4238e-01],\n",
       "            [8.3872e-02, 1.4389e-01, 4.4781e-02,  ..., 1.0849e-01,\n",
       "             1.3764e-01, 7.4951e-02],\n",
       "            [1.4695e-01, 3.8916e-02, 5.2062e-02,  ..., 1.1652e-01,\n",
       "             5.9127e-02, 2.6031e-01],\n",
       "            ...,\n",
       "            [9.5946e-02, 1.0981e-01, 1.3782e-01,  ..., 1.5315e-01,\n",
       "             9.4929e-02, 5.1951e-02],\n",
       "            [4.7241e-02, 1.1906e-01, 8.8495e-02,  ..., 9.3691e-02,\n",
       "             9.1235e-02, 5.9144e-03],\n",
       "            [7.5462e-02, 2.1825e-03, 2.5422e-03,  ..., 4.6770e-04,\n",
       "             1.0242e-03, 2.4606e-01]],\n",
       "  \n",
       "           [[6.0287e-01, 3.6737e-02, 2.4496e-02,  ..., 2.3933e-02,\n",
       "             4.6374e-02, 4.4747e-02],\n",
       "            [9.0318e-03, 7.1680e-02, 4.7949e-01,  ..., 1.0924e-01,\n",
       "             6.3975e-02, 6.7686e-03],\n",
       "            [1.5300e-01, 1.0412e-01, 7.8116e-02,  ..., 6.2889e-02,\n",
       "             4.2682e-02, 1.3585e-02],\n",
       "            ...,\n",
       "            [1.0592e-02, 2.6376e-02, 1.7783e-01,  ..., 1.1949e-01,\n",
       "             7.5171e-02, 3.1968e-02],\n",
       "            [3.3551e-01, 1.2324e-02, 3.0393e-02,  ..., 1.4306e-01,\n",
       "             6.1716e-02, 4.8924e-02],\n",
       "            [2.4574e-02, 2.1677e-02, 3.7307e-02,  ..., 1.4413e-01,\n",
       "             3.6552e-01, 2.2736e-02]],\n",
       "  \n",
       "           [[2.7476e-01, 4.6880e-02, 8.1602e-02,  ..., 8.5515e-02,\n",
       "             7.8442e-02, 1.2251e-01],\n",
       "            [1.5323e-02, 3.6419e-02, 8.8378e-02,  ..., 1.4155e-01,\n",
       "             1.6769e-01, 4.7212e-02],\n",
       "            [2.5216e-02, 3.0312e-01, 4.1170e-02,  ..., 6.9214e-02,\n",
       "             1.5519e-01, 2.4052e-02],\n",
       "            ...,\n",
       "            [3.0895e-02, 7.2778e-02, 6.9804e-02,  ..., 1.1271e-01,\n",
       "             2.0327e-01, 4.8202e-02],\n",
       "            [1.7509e-01, 1.0610e-01, 1.2156e-01,  ..., 1.1652e-01,\n",
       "             8.8054e-02, 5.5159e-02],\n",
       "            [5.7568e-02, 5.3003e-02, 1.0290e-01,  ..., 1.2140e-01,\n",
       "             1.3434e-01, 6.8908e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.1156e-01, 4.4743e-02, 1.9480e-01,  ..., 5.9057e-02,\n",
       "             1.3463e-01, 7.0947e-02],\n",
       "            [3.0255e-02, 5.3784e-03, 3.7861e-01,  ..., 5.7482e-02,\n",
       "             1.9601e-01, 8.6981e-02],\n",
       "            [4.4804e-02, 8.4706e-02, 1.6546e-02,  ..., 1.3944e-01,\n",
       "             4.3316e-02, 1.0173e-02],\n",
       "            ...,\n",
       "            [5.8833e-02, 7.9855e-02, 2.1387e-01,  ..., 2.0083e-02,\n",
       "             1.3614e-01, 1.1643e-01],\n",
       "            [4.0187e-02, 1.4008e-01, 2.2335e-01,  ..., 8.4339e-02,\n",
       "             5.2985e-02, 9.9558e-02],\n",
       "            [9.1037e-03, 4.2552e-02, 1.1045e-01,  ..., 1.9116e-01,\n",
       "             1.1822e-01, 9.4096e-03]],\n",
       "  \n",
       "           [[6.7763e-01, 1.4411e-02, 2.1511e-02,  ..., 1.0583e-02,\n",
       "             1.6052e-02, 9.3719e-02],\n",
       "            [2.1814e-01, 4.3080e-01, 5.8597e-03,  ..., 4.0788e-02,\n",
       "             2.9253e-02, 7.1495e-02],\n",
       "            [5.1661e-01, 6.4159e-03, 2.8140e-01,  ..., 4.4561e-02,\n",
       "             5.0402e-02, 4.6282e-02],\n",
       "            ...,\n",
       "            [2.9065e-01, 3.7779e-02, 7.2944e-02,  ..., 4.4565e-01,\n",
       "             2.8632e-02, 1.6930e-02],\n",
       "            [5.3708e-01, 4.3452e-02, 5.9321e-02,  ..., 2.8392e-02,\n",
       "             4.9151e-02, 2.6363e-02],\n",
       "            [4.9109e-01, 6.7357e-03, 2.5982e-02,  ..., 2.6541e-03,\n",
       "             3.2785e-03, 2.5380e-02]],\n",
       "  \n",
       "           [[3.5522e-06, 4.9707e-07, 8.3160e-06,  ..., 7.8195e-06,\n",
       "             6.9817e-07, 5.1106e-01],\n",
       "            [5.5630e-03, 1.4290e-01, 2.7742e-02,  ..., 3.3113e-02,\n",
       "             5.5543e-02, 5.7211e-03],\n",
       "            [1.5888e-02, 1.1195e-01, 5.4727e-02,  ..., 2.9187e-01,\n",
       "             6.4662e-02, 1.7864e-02],\n",
       "            ...,\n",
       "            [2.5787e-02, 1.6948e-01, 1.0317e-01,  ..., 1.8068e-01,\n",
       "             1.0882e-01, 9.9156e-03],\n",
       "            [6.6963e-03, 1.7017e-01, 1.3463e-01,  ..., 2.2016e-01,\n",
       "             8.9848e-02, 4.4224e-03],\n",
       "            [3.6613e-01, 5.3030e-02, 6.7447e-02,  ..., 9.0106e-02,\n",
       "             3.5005e-02, 1.3373e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[7.7785e-01, 1.4572e-02, 1.2244e-02,  ..., 8.2570e-03,\n",
       "             6.0297e-03, 6.9752e-02],\n",
       "            [8.2581e-02, 3.8977e-02, 1.0240e-01,  ..., 1.1403e-01,\n",
       "             9.6197e-03, 1.3398e-01],\n",
       "            [9.7469e-02, 1.4421e-01, 1.2016e-01,  ..., 1.0706e-01,\n",
       "             9.3255e-03, 5.3770e-02],\n",
       "            ...,\n",
       "            [2.4550e-01, 2.9649e-02, 7.3815e-02,  ..., 8.2574e-02,\n",
       "             1.1766e-02, 1.1784e-01],\n",
       "            [1.9736e-02, 1.1190e-02, 1.0501e-02,  ..., 2.1628e-02,\n",
       "             3.0711e-01, 5.8515e-02],\n",
       "            [7.3262e-01, 2.3057e-03, 2.2061e-03,  ..., 2.5213e-03,\n",
       "             2.4113e-03, 1.3642e-01]],\n",
       "  \n",
       "           [[7.6070e-01, 1.6413e-02, 1.5246e-02,  ..., 1.1542e-02,\n",
       "             7.5385e-03, 6.6593e-02],\n",
       "            [2.2382e-01, 1.0827e-02, 5.2832e-02,  ..., 5.6080e-03,\n",
       "             2.1080e-04, 4.4006e-03],\n",
       "            [5.6333e-02, 2.4629e-02, 1.6154e-02,  ..., 2.4824e-02,\n",
       "             3.0409e-05, 8.0148e-03],\n",
       "            ...,\n",
       "            [7.4265e-02, 1.0879e-03, 5.7768e-04,  ..., 7.2031e-02,\n",
       "             1.9089e-03, 7.7983e-01],\n",
       "            [3.7102e-02, 2.4718e-03, 7.4125e-03,  ..., 5.2176e-02,\n",
       "             2.4058e-02, 7.9676e-01],\n",
       "            [9.1336e-01, 1.0955e-04, 1.3769e-04,  ..., 2.7327e-03,\n",
       "             7.6982e-04, 6.8335e-02]],\n",
       "  \n",
       "           [[7.7167e-01, 3.4156e-03, 8.4817e-03,  ..., 1.6589e-02,\n",
       "             1.6453e-02, 6.7186e-02],\n",
       "            [8.1739e-01, 7.1240e-03, 1.8928e-04,  ..., 3.0996e-05,\n",
       "             7.2290e-05, 2.2696e-03],\n",
       "            [7.8193e-01, 3.4994e-02, 4.6030e-03,  ..., 7.9792e-06,\n",
       "             2.7473e-05, 2.1910e-03],\n",
       "            ...,\n",
       "            [6.1367e-01, 3.3223e-05, 5.1743e-03,  ..., 1.3713e-02,\n",
       "             2.4782e-03, 1.8693e-01],\n",
       "            [4.3043e-01, 1.7413e-05, 2.2346e-03,  ..., 1.5760e-01,\n",
       "             1.1289e-03, 2.7133e-01],\n",
       "            [5.7181e-01, 1.0325e-05, 1.7322e-05,  ..., 1.1127e-02,\n",
       "             4.1362e-04, 2.8770e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[5.3223e-01, 6.2364e-03, 2.3238e-02,  ..., 4.2410e-02,\n",
       "             1.0945e-02, 1.7513e-01],\n",
       "            [6.1376e-01, 3.2877e-04, 2.0436e-02,  ..., 9.5332e-02,\n",
       "             2.5097e-03, 8.9224e-02],\n",
       "            [3.9677e-01, 2.6150e-03, 6.1321e-03,  ..., 2.7266e-01,\n",
       "             2.4662e-03, 1.4208e-01],\n",
       "            ...,\n",
       "            [7.0400e-01, 7.9696e-04, 1.8850e-02,  ..., 1.0045e-01,\n",
       "             3.4059e-03, 1.2579e-01],\n",
       "            [4.1941e-01, 3.8940e-02, 7.9082e-03,  ..., 8.2167e-02,\n",
       "             7.8732e-02, 1.3808e-01],\n",
       "            [7.6270e-01, 3.8375e-03, 9.2479e-03,  ..., 4.5329e-02,\n",
       "             1.4272e-03, 8.6861e-02]],\n",
       "  \n",
       "           [[6.0139e-01, 6.4970e-04, 7.2050e-04,  ..., 2.1768e-03,\n",
       "             8.9302e-04, 2.1165e-01],\n",
       "            [5.7589e-01, 4.6619e-02, 3.1229e-02,  ..., 4.5692e-02,\n",
       "             6.4406e-03, 6.9555e-02],\n",
       "            [7.3632e-01, 1.3045e-03, 2.4630e-02,  ..., 5.6094e-02,\n",
       "             1.0625e-03, 8.1213e-02],\n",
       "            ...,\n",
       "            [8.4529e-01, 1.8902e-03, 1.2850e-02,  ..., 2.3670e-02,\n",
       "             1.7565e-03, 4.4960e-02],\n",
       "            [2.1195e-01, 1.4679e-02, 1.2759e-02,  ..., 3.0463e-02,\n",
       "             1.1231e-01, 9.0189e-02],\n",
       "            [4.9968e-01, 4.8855e-03, 4.2741e-03,  ..., 3.9530e-03,\n",
       "             3.2132e-02, 1.1095e-01]],\n",
       "  \n",
       "           [[8.2036e-01, 1.9283e-02, 1.2223e-02,  ..., 2.1116e-02,\n",
       "             2.3895e-03, 2.8805e-02],\n",
       "            [1.3599e-01, 4.6340e-04, 8.5838e-01,  ..., 3.6702e-07,\n",
       "             9.9799e-12, 1.4711e-04],\n",
       "            [7.5520e-02, 1.1849e-05, 3.0375e-03,  ..., 1.2654e-03,\n",
       "             1.1898e-08, 1.4770e-08],\n",
       "            ...,\n",
       "            [4.2691e-01, 2.7173e-05, 2.7053e-08,  ..., 7.2475e-02,\n",
       "             2.1251e-01, 4.4523e-02],\n",
       "            [1.6846e-05, 1.9642e-05, 5.5244e-07,  ..., 9.1694e-04,\n",
       "             2.3351e-02, 9.7317e-01],\n",
       "            [9.9373e-01, 2.1309e-07, 5.8203e-07,  ..., 9.2916e-05,\n",
       "             4.1373e-09, 6.0195e-03]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[3.9928e-01, 3.7532e-02, 4.1292e-02,  ..., 6.6946e-02,\n",
       "             4.9187e-02, 1.4692e-01],\n",
       "            [9.5079e-02, 2.2240e-01, 6.3844e-03,  ..., 1.2638e-01,\n",
       "             2.3315e-02, 2.0878e-01],\n",
       "            [1.1874e-01, 8.5896e-02, 1.2594e-01,  ..., 1.1554e-02,\n",
       "             1.6454e-02, 1.8768e-01],\n",
       "            ...,\n",
       "            [2.5482e-01, 6.7909e-03, 4.9008e-03,  ..., 9.4320e-02,\n",
       "             1.1679e-02, 2.9934e-01],\n",
       "            [4.0222e-01, 2.5911e-02, 7.3017e-02,  ..., 8.9724e-02,\n",
       "             5.6775e-02, 9.1222e-02],\n",
       "            [6.8724e-01, 2.6329e-02, 7.6126e-03,  ..., 1.3281e-02,\n",
       "             1.4445e-02, 1.0617e-01]],\n",
       "  \n",
       "           [[5.5472e-01, 2.3116e-02, 2.9250e-02,  ..., 2.4221e-02,\n",
       "             1.4346e-02, 1.4648e-01],\n",
       "            [8.5721e-01, 4.7525e-03, 1.8844e-04,  ..., 2.4426e-04,\n",
       "             5.1681e-04, 5.1136e-02],\n",
       "            [2.7756e-01, 3.2662e-01, 1.2369e-02,  ..., 3.2667e-05,\n",
       "             1.9971e-04, 1.7228e-01],\n",
       "            ...,\n",
       "            [1.3604e-01, 2.4428e-02, 5.4073e-02,  ..., 1.4818e-02,\n",
       "             8.6963e-03, 3.7621e-01],\n",
       "            [9.3871e-02, 2.9487e-03, 1.3296e-01,  ..., 4.5364e-01,\n",
       "             4.6323e-02, 1.1138e-01],\n",
       "            [7.6801e-01, 4.5387e-03, 1.6993e-03,  ..., 5.0005e-03,\n",
       "             2.0939e-02, 8.7778e-02]],\n",
       "  \n",
       "           [[2.6583e-01, 8.9985e-02, 3.3792e-02,  ..., 1.0095e-01,\n",
       "             6.4259e-02, 9.7060e-02],\n",
       "            [1.8080e-01, 4.2464e-01, 7.2395e-03,  ..., 2.7210e-02,\n",
       "             2.9048e-02, 9.0553e-02],\n",
       "            [1.9090e-01, 1.5276e-02, 4.7777e-01,  ..., 4.1431e-02,\n",
       "             3.3100e-02, 9.6557e-02],\n",
       "            ...,\n",
       "            [1.2944e-01, 5.1233e-02, 6.3114e-02,  ..., 3.2284e-01,\n",
       "             3.5417e-02, 1.4511e-01],\n",
       "            [5.7248e-02, 1.4329e-01, 4.1424e-02,  ..., 1.1530e-01,\n",
       "             1.4093e-01, 1.2626e-01],\n",
       "            [2.9104e-01, 3.7041e-02, 3.9049e-02,  ..., 3.9595e-02,\n",
       "             1.6422e-02, 2.3891e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[2.5523e-01, 2.7466e-02, 2.6950e-02,  ..., 3.2811e-02,\n",
       "             1.3664e-02, 3.0391e-01],\n",
       "            [1.4874e-01, 5.8384e-02, 1.6575e-01,  ..., 1.2560e-01,\n",
       "             1.5963e-02, 1.8627e-01],\n",
       "            [1.7289e-01, 8.3622e-02, 5.2504e-02,  ..., 1.0967e-01,\n",
       "             2.8296e-02, 1.0925e-01],\n",
       "            ...,\n",
       "            [1.3117e-01, 1.7492e-02, 2.4429e-01,  ..., 3.7017e-02,\n",
       "             8.7906e-03, 2.0587e-01],\n",
       "            [1.9156e-01, 6.7939e-02, 9.7396e-02,  ..., 1.7787e-01,\n",
       "             5.9162e-02, 1.3072e-01],\n",
       "            [2.1638e-01, 1.6304e-02, 3.8633e-03,  ..., 5.2118e-03,\n",
       "             1.3728e-02, 3.7277e-01]],\n",
       "  \n",
       "           [[2.6270e-01, 1.1715e-02, 1.5031e-02,  ..., 3.5951e-02,\n",
       "             1.6571e-02, 3.0024e-01],\n",
       "            [1.0153e-01, 5.3808e-02, 9.4050e-02,  ..., 7.5723e-02,\n",
       "             2.4225e-02, 2.2622e-01],\n",
       "            [7.5866e-02, 7.4413e-02, 7.2141e-02,  ..., 1.5237e-01,\n",
       "             2.0807e-02, 1.2543e-01],\n",
       "            ...,\n",
       "            [1.1184e-01, 1.7780e-02, 8.6698e-02,  ..., 1.2562e-01,\n",
       "             2.5807e-02, 2.4820e-01],\n",
       "            [1.2794e-01, 5.3707e-02, 3.7185e-02,  ..., 6.3800e-02,\n",
       "             5.6010e-02, 2.7401e-01],\n",
       "            [5.4166e-01, 1.8961e-03, 4.2039e-04,  ..., 8.1314e-04,\n",
       "             5.2290e-03, 2.2561e-01]],\n",
       "  \n",
       "           [[2.8866e-02, 5.5977e-02, 7.1313e-01,  ..., 1.1409e-01,\n",
       "             2.0408e-02, 1.1892e-03],\n",
       "            [1.6897e-01, 4.9964e-02, 4.6541e-02,  ..., 2.8444e-01,\n",
       "             1.3910e-01, 3.4543e-02],\n",
       "            [4.0405e-02, 1.2116e-01, 1.3033e-01,  ..., 5.1072e-01,\n",
       "             2.1714e-02, 1.5140e-03],\n",
       "            ...,\n",
       "            [7.7489e-02, 2.4259e-01, 1.2656e-01,  ..., 3.6079e-02,\n",
       "             2.0296e-01, 7.8016e-03],\n",
       "            [9.4237e-02, 8.4987e-02, 8.1608e-02,  ..., 2.5828e-01,\n",
       "             1.6964e-01, 2.3329e-02],\n",
       "            [8.0214e-02, 5.0564e-03, 2.4843e-03,  ..., 1.7077e-03,\n",
       "             1.9049e-03, 4.8227e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[1.2595e-02, 7.2645e-02, 1.0020e-02,  ..., 3.5858e-03,\n",
       "             3.2862e-03, 4.2941e-01],\n",
       "            [2.1958e-02, 1.4697e-02, 4.3990e-03,  ..., 5.4454e-03,\n",
       "             2.3363e-02, 4.4050e-01],\n",
       "            [6.2652e-02, 6.4051e-03, 3.9145e-02,  ..., 1.7196e-02,\n",
       "             1.7819e-02, 3.9621e-01],\n",
       "            ...,\n",
       "            [2.4891e-02, 7.0680e-04, 1.8372e-03,  ..., 1.5989e-03,\n",
       "             7.0069e-03, 4.7059e-01],\n",
       "            [3.0284e-02, 2.2375e-02, 6.6770e-03,  ..., 2.6413e-02,\n",
       "             4.2275e-02, 4.0309e-01],\n",
       "            [5.4433e-02, 2.8899e-03, 3.2975e-03,  ..., 2.0939e-03,\n",
       "             1.1482e-02, 4.4912e-01]],\n",
       "  \n",
       "           [[1.4296e-01, 1.8888e-01, 1.4572e-01,  ..., 6.6102e-02,\n",
       "             1.4194e-01, 4.5379e-02],\n",
       "            [9.6297e-02, 1.6306e-02, 5.0932e-03,  ..., 1.2181e-02,\n",
       "             2.4240e-01, 1.4400e-01],\n",
       "            [1.5129e-01, 4.2437e-02, 2.2896e-02,  ..., 9.2618e-03,\n",
       "             3.9314e-02, 3.0358e-01],\n",
       "            ...,\n",
       "            [1.1809e-01, 4.9020e-02, 2.6903e-02,  ..., 1.5884e-02,\n",
       "             3.6379e-02, 3.0560e-01],\n",
       "            [8.3336e-02, 2.4184e-02, 5.7862e-03,  ..., 7.7476e-03,\n",
       "             1.8260e-01, 1.9019e-01],\n",
       "            [1.2615e-02, 2.5715e-02, 2.0972e-02,  ..., 1.6939e-02,\n",
       "             3.6698e-03, 4.4231e-01]],\n",
       "  \n",
       "           [[9.0345e-02, 4.6859e-02, 1.3390e-01,  ..., 5.8353e-02,\n",
       "             1.7179e-02, 2.5168e-01],\n",
       "            [8.6460e-02, 1.4768e-01, 1.3375e-01,  ..., 3.6611e-02,\n",
       "             2.2305e-02, 1.5110e-01],\n",
       "            [8.8708e-02, 4.3165e-02, 1.3383e-01,  ..., 1.1249e-01,\n",
       "             6.8483e-03, 2.6595e-01],\n",
       "            ...,\n",
       "            [9.0020e-02, 8.5217e-02, 9.9565e-02,  ..., 1.0723e-01,\n",
       "             1.2650e-02, 2.0568e-01],\n",
       "            [8.1877e-02, 1.2095e-01, 1.3340e-01,  ..., 1.1623e-01,\n",
       "             1.9599e-02, 1.2858e-01],\n",
       "            [3.3857e-03, 3.7826e-03, 1.2561e-03,  ..., 8.3454e-04,\n",
       "             7.9830e-03, 4.7996e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.3349e-02, 4.0147e-02, 6.7596e-03,  ..., 2.5706e-02,\n",
       "             1.9266e-02, 4.1828e-01],\n",
       "            [2.2914e-02, 1.6257e-02, 1.0153e-02,  ..., 8.1523e-02,\n",
       "             4.9325e-02, 3.8340e-01],\n",
       "            [1.7280e-02, 2.0053e-02, 2.3852e-01,  ..., 9.5490e-03,\n",
       "             1.6838e-02, 3.1323e-01],\n",
       "            ...,\n",
       "            [1.4137e-02, 7.5508e-03, 2.0533e-02,  ..., 1.2832e-01,\n",
       "             2.4044e-02, 3.7180e-01],\n",
       "            [5.2353e-02, 5.4035e-02, 6.9993e-03,  ..., 1.4993e-02,\n",
       "             2.4734e-02, 3.9043e-01],\n",
       "            [1.0894e-02, 2.2821e-02, 1.1785e-02,  ..., 1.4487e-02,\n",
       "             1.7011e-02, 4.4112e-01]],\n",
       "  \n",
       "           [[1.0624e-02, 5.1978e-02, 3.1015e-02,  ..., 5.3454e-03,\n",
       "             3.0893e-03, 4.3526e-01],\n",
       "            [5.5496e-02, 1.0784e-01, 6.4979e-02,  ..., 2.2614e-02,\n",
       "             1.2371e-02, 3.2939e-01],\n",
       "            [1.4694e-02, 1.6445e-01, 8.1807e-02,  ..., 3.0856e-02,\n",
       "             1.3065e-02, 2.9012e-01],\n",
       "            ...,\n",
       "            [6.1210e-02, 4.0289e-02, 8.0051e-02,  ..., 2.1382e-02,\n",
       "             1.4672e-02, 3.2309e-01],\n",
       "            [2.8106e-02, 3.4418e-02, 2.3407e-02,  ..., 4.6276e-02,\n",
       "             2.9405e-02, 3.6657e-01],\n",
       "            [7.6835e-03, 3.5295e-03, 1.0818e-03,  ..., 1.4852e-03,\n",
       "             2.4156e-03, 4.8144e-01]],\n",
       "  \n",
       "           [[1.0427e-01, 4.9471e-02, 4.5923e-02,  ..., 9.2859e-02,\n",
       "             3.7561e-01, 3.6502e-02],\n",
       "            [5.7388e-01, 9.6279e-04, 5.6001e-06,  ..., 5.0400e-08,\n",
       "             1.6347e-08, 1.9603e-01],\n",
       "            [1.9857e-04, 9.7348e-01, 2.5724e-04,  ..., 1.9704e-07,\n",
       "             6.8336e-08, 1.4408e-02],\n",
       "            ...,\n",
       "            [3.4566e-06, 1.8537e-04, 2.7353e-03,  ..., 1.3288e-03,\n",
       "             7.0419e-05, 7.0408e-04],\n",
       "            [5.7357e-04, 2.0278e-05, 1.8027e-03,  ..., 9.9208e-01,\n",
       "             1.6268e-03, 3.4621e-04],\n",
       "            [8.2837e-01, 1.5244e-02, 3.4747e-03,  ..., 7.6775e-03,\n",
       "             1.7927e-02, 3.9017e-02]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[7.0630e-03, 2.5288e-03, 4.0377e-01,  ..., 2.4620e-02,\n",
       "             3.7845e-02, 2.1197e-01],\n",
       "            [2.7603e-03, 7.7167e-04, 6.9766e-03,  ..., 7.0234e-06,\n",
       "             2.2768e-02, 4.8694e-01],\n",
       "            [9.1635e-03, 3.6142e-05, 9.1605e-01,  ..., 7.6025e-06,\n",
       "             3.1868e-03, 3.4697e-02],\n",
       "            ...,\n",
       "            [2.1650e-02, 1.0275e-05, 1.7815e-04,  ..., 5.2217e-02,\n",
       "             1.9561e-02, 4.4547e-01],\n",
       "            [2.9394e-02, 1.5896e-02, 1.9978e-02,  ..., 1.0747e-02,\n",
       "             2.4330e-01, 2.3924e-01],\n",
       "            [1.5424e-02, 1.7420e-03, 3.5012e-03,  ..., 3.9790e-03,\n",
       "             5.9754e-03, 4.7918e-01]],\n",
       "  \n",
       "           [[1.0627e-01, 4.7449e-02, 6.5220e-02,  ..., 4.0213e-02,\n",
       "             4.8418e-03, 3.1001e-01],\n",
       "            [1.4848e-02, 1.4182e-01, 6.6822e-02,  ..., 7.4250e-02,\n",
       "             5.4552e-02, 1.9700e-01],\n",
       "            [3.3512e-03, 1.4082e-02, 1.5207e-02,  ..., 8.9524e-03,\n",
       "             1.5124e-02, 4.5353e-01],\n",
       "            ...,\n",
       "            [1.8337e-03, 9.4365e-03, 1.9132e-02,  ..., 1.0784e-02,\n",
       "             2.0890e-02, 4.4661e-01],\n",
       "            [2.0257e-03, 6.0091e-02, 3.3837e-02,  ..., 1.3706e-01,\n",
       "             9.9488e-02, 2.5205e-01],\n",
       "            [1.2145e-03, 1.2123e-03, 4.5917e-04,  ..., 5.3298e-04,\n",
       "             2.8368e-03, 4.9500e-01]],\n",
       "  \n",
       "           [[1.2761e-03, 1.6972e-02, 1.9407e-02,  ..., 6.1034e-03,\n",
       "             3.0916e-03, 4.6424e-01],\n",
       "            [1.2983e-02, 5.3431e-03, 2.9549e-03,  ..., 3.0706e-03,\n",
       "             1.5600e-03, 4.6879e-01],\n",
       "            [5.8123e-02, 3.6838e-01, 5.5853e-02,  ..., 8.9268e-03,\n",
       "             4.5306e-03, 2.2798e-01],\n",
       "            ...,\n",
       "            [1.1165e-02, 3.9704e-02, 1.7029e-01,  ..., 3.6747e-02,\n",
       "             2.8428e-03, 4.6822e-02],\n",
       "            [3.5504e-03, 5.0754e-02, 1.0329e-01,  ..., 2.1023e-01,\n",
       "             4.3352e-02, 1.4293e-01],\n",
       "            [4.9337e-03, 4.3631e-03, 7.2381e-03,  ..., 3.4247e-03,\n",
       "             1.5601e-02, 4.6366e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[6.3578e-03, 6.3831e-02, 3.1948e-02,  ..., 7.5814e-03,\n",
       "             1.2332e-02, 3.8075e-01],\n",
       "            [4.2312e-03, 3.4211e-02, 1.8545e-02,  ..., 1.0631e-02,\n",
       "             7.0426e-02, 3.2660e-01],\n",
       "            [3.6124e-02, 6.6665e-02, 1.9179e-01,  ..., 1.1782e-02,\n",
       "             4.5647e-02, 1.2303e-01],\n",
       "            ...,\n",
       "            [4.7680e-02, 6.2660e-02, 6.8272e-02,  ..., 2.8226e-02,\n",
       "             1.9287e-01, 1.3705e-01],\n",
       "            [8.7555e-03, 1.1523e-01, 3.6612e-01,  ..., 8.7214e-02,\n",
       "             2.5697e-02, 8.1331e-02],\n",
       "            [9.7535e-04, 1.1856e-02, 1.6371e-02,  ..., 2.7351e-03,\n",
       "             2.0794e-03, 4.6969e-01]],\n",
       "  \n",
       "           [[4.3358e-02, 2.4363e-02, 4.6821e-02,  ..., 8.6550e-03,\n",
       "             1.2892e-02, 4.0259e-01],\n",
       "            [1.0600e-02, 1.2543e-01, 1.3224e-01,  ..., 8.3128e-02,\n",
       "             1.2699e-01, 7.2619e-02],\n",
       "            [5.3714e-02, 8.9863e-02, 1.1344e-01,  ..., 1.1514e-01,\n",
       "             1.0428e-01, 1.1260e-01],\n",
       "            ...,\n",
       "            [2.9760e-03, 5.1230e-02, 9.4943e-02,  ..., 5.0890e-02,\n",
       "             1.0753e-01, 2.7089e-01],\n",
       "            [2.2175e-02, 2.0612e-01, 2.5287e-02,  ..., 5.4194e-02,\n",
       "             1.6817e-01, 1.3616e-01],\n",
       "            [1.9348e-04, 1.0506e-03, 2.3910e-04,  ..., 1.2184e-04,\n",
       "             1.7208e-04, 4.9893e-01]],\n",
       "  \n",
       "           [[1.0320e-02, 1.6505e-01, 3.7414e-02,  ..., 1.1820e-01,\n",
       "             8.1637e-02, 2.2333e-01],\n",
       "            [1.1040e-01, 5.9435e-02, 4.5780e-02,  ..., 3.6899e-02,\n",
       "             7.4470e-02, 2.1672e-01],\n",
       "            [5.3499e-02, 1.2435e-01, 2.7916e-03,  ..., 1.1946e-01,\n",
       "             4.4362e-02, 2.1302e-01],\n",
       "            ...,\n",
       "            [1.6122e-02, 3.3179e-02, 2.3169e-01,  ..., 4.2950e-03,\n",
       "             1.2130e-01, 2.2263e-01],\n",
       "            [1.2084e-02, 5.8926e-02, 9.9595e-02,  ..., 1.0484e-01,\n",
       "             1.0741e-01, 2.2735e-01],\n",
       "            [4.2404e-03, 2.9511e-03, 9.6081e-03,  ..., 2.5457e-03,\n",
       "             9.0463e-03, 4.8058e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[5.9708e-03, 3.0753e-02, 2.8950e-02,  ..., 1.0897e-02,\n",
       "             4.9062e-04, 3.3720e-01],\n",
       "            [4.4621e-04, 3.1806e-04, 8.5281e-02,  ..., 1.9910e-02,\n",
       "             2.2407e-02, 4.0329e-01],\n",
       "            [4.3325e-03, 4.5773e-02, 2.6218e-03,  ..., 7.1330e-02,\n",
       "             2.6692e-02, 3.5323e-01],\n",
       "            ...,\n",
       "            [1.3722e-03, 3.6122e-02, 1.1860e-01,  ..., 1.9268e-03,\n",
       "             5.0207e-03, 3.7906e-01],\n",
       "            [1.2368e-03, 1.0382e-01, 1.5774e-01,  ..., 1.9401e-02,\n",
       "             6.7673e-03, 2.6290e-01],\n",
       "            [3.9525e-04, 9.0971e-04, 7.6477e-04,  ..., 5.4169e-04,\n",
       "             6.7090e-04, 4.9732e-01]],\n",
       "  \n",
       "           [[1.5810e-02, 9.5414e-02, 1.3683e-04,  ..., 8.6955e-04,\n",
       "             5.1244e-04, 4.3759e-01],\n",
       "            [7.5128e-04, 2.2356e-03, 2.4263e-08,  ..., 9.4288e-08,\n",
       "             6.3765e-08, 4.8364e-01],\n",
       "            [4.6837e-04, 9.5768e-01, 5.2386e-04,  ..., 9.4536e-06,\n",
       "             8.5308e-07, 1.9021e-02],\n",
       "            ...,\n",
       "            [5.2321e-07, 2.1031e-04, 1.2393e-05,  ..., 1.1317e-04,\n",
       "             5.5765e-08, 8.6255e-05],\n",
       "            [5.7706e-05, 6.8144e-05, 1.0755e-02,  ..., 9.4585e-01,\n",
       "             6.6606e-04, 1.3355e-02],\n",
       "            [3.5316e-01, 1.8074e-02, 1.6626e-02,  ..., 5.6199e-03,\n",
       "             1.5771e-02, 2.8041e-01]],\n",
       "  \n",
       "           [[2.1061e-03, 1.4512e-03, 8.6405e-03,  ..., 4.2167e-03,\n",
       "             1.7356e-03, 4.8795e-01],\n",
       "            [6.2622e-03, 8.3940e-03, 4.1985e-02,  ..., 6.8188e-02,\n",
       "             2.2966e-02, 3.9078e-01],\n",
       "            [3.8365e-02, 3.9040e-02, 1.6086e-02,  ..., 5.8967e-03,\n",
       "             2.6392e-02, 3.9047e-01],\n",
       "            ...,\n",
       "            [6.6157e-02, 5.8991e-02, 1.0815e-02,  ..., 5.5789e-03,\n",
       "             4.1860e-02, 1.9173e-01],\n",
       "            [1.5138e-02, 1.8689e-02, 7.1747e-03,  ..., 2.1742e-02,\n",
       "             1.1115e-02, 4.1978e-01],\n",
       "            [9.9172e-04, 8.7067e-04, 1.7538e-03,  ..., 1.5897e-03,\n",
       "             7.7783e-04, 4.9369e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.3153e-03, 1.8666e-02, 1.6885e-02,  ..., 1.2150e-03,\n",
       "             2.2898e-03, 4.6030e-01],\n",
       "            [4.4986e-03, 9.2701e-03, 3.0617e-02,  ..., 7.0437e-04,\n",
       "             3.2790e-03, 4.5793e-01],\n",
       "            [5.6986e-02, 2.6365e-02, 1.3237e-02,  ..., 6.6206e-04,\n",
       "             9.1137e-03, 3.6711e-01],\n",
       "            ...,\n",
       "            [5.9066e-02, 3.5189e-03, 1.7718e-02,  ..., 1.7204e-02,\n",
       "             1.2239e-02, 4.0236e-01],\n",
       "            [2.3048e-02, 1.7636e-02, 1.5588e-02,  ..., 1.9793e-02,\n",
       "             2.3795e-01, 2.3451e-01],\n",
       "            [1.0646e-02, 5.0687e-03, 3.2620e-03,  ..., 7.7601e-04,\n",
       "             5.8305e-03, 4.7518e-01]],\n",
       "  \n",
       "           [[2.0100e-03, 1.9294e-02, 1.4675e-01,  ..., 4.4247e-02,\n",
       "             4.9884e-04, 3.8110e-01],\n",
       "            [1.2756e-02, 1.2081e-02, 1.7204e-02,  ..., 1.1704e-02,\n",
       "             2.8872e-03, 4.6526e-01],\n",
       "            [4.7614e-02, 2.6035e-02, 1.2754e-02,  ..., 4.9368e-04,\n",
       "             4.6935e-04, 4.5072e-01],\n",
       "            ...,\n",
       "            [9.6660e-04, 3.8412e-03, 2.5945e-02,  ..., 4.1806e-03,\n",
       "             1.1954e-04, 4.7313e-01],\n",
       "            [1.1115e-02, 3.4055e-03, 1.9891e-02,  ..., 4.4992e-01,\n",
       "             8.7797e-03, 2.3985e-01],\n",
       "            [1.2210e-02, 6.7585e-03, 3.3027e-03,  ..., 1.8451e-03,\n",
       "             3.4400e-02, 4.5172e-01]],\n",
       "  \n",
       "           [[1.3137e-02, 2.1963e-02, 4.7849e-02,  ..., 4.0506e-02,\n",
       "             1.0908e-02, 3.6644e-01],\n",
       "            [3.0943e-02, 3.9036e-02, 4.3363e-02,  ..., 1.6620e-01,\n",
       "             3.8908e-02, 2.3145e-01],\n",
       "            [7.4348e-03, 2.7088e-02, 1.8262e-02,  ..., 1.8175e-01,\n",
       "             3.8049e-02, 2.6495e-01],\n",
       "            ...,\n",
       "            [4.7615e-03, 4.1204e-03, 5.4832e-03,  ..., 1.3289e-02,\n",
       "             9.3593e-03, 4.7670e-01],\n",
       "            [1.2880e-02, 2.3151e-02, 1.6572e-02,  ..., 9.9641e-03,\n",
       "             1.8056e-02, 4.3936e-01],\n",
       "            [4.6170e-03, 1.6940e-03, 9.2928e-04,  ..., 1.1630e-03,\n",
       "             1.4371e-03, 4.9186e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[1.0092e-02, 1.8865e-01, 1.2907e-01,  ..., 5.4121e-02,\n",
       "             2.2104e-01, 2.7271e-02],\n",
       "            [2.4252e-02, 1.0654e-02, 8.7093e-03,  ..., 1.8866e-03,\n",
       "             1.9216e-02, 4.4065e-01],\n",
       "            [3.0218e-02, 1.1614e-03, 1.5458e-03,  ..., 8.3238e-04,\n",
       "             6.7463e-03, 4.6715e-01],\n",
       "            ...,\n",
       "            [6.7750e-02, 4.6138e-03, 8.8990e-03,  ..., 2.9641e-04,\n",
       "             1.0806e-02, 4.2655e-01],\n",
       "            [4.1811e-01, 9.2597e-04, 3.0546e-03,  ..., 2.0105e-03,\n",
       "             3.8345e-02, 2.4723e-01],\n",
       "            [1.6614e-02, 7.3529e-03, 7.0087e-03,  ..., 6.8328e-03,\n",
       "             2.6081e-03, 4.7229e-01]],\n",
       "  \n",
       "           [[1.9538e-02, 1.1302e-02, 1.5920e-01,  ..., 2.5789e-01,\n",
       "             1.3860e-01, 9.7835e-02],\n",
       "            [1.6969e-02, 3.4049e-04, 6.4518e-04,  ..., 1.4336e-05,\n",
       "             9.2951e-05, 4.8918e-01],\n",
       "            [4.0480e-02, 8.6137e-01, 3.7759e-03,  ..., 2.9355e-04,\n",
       "             6.9024e-05, 4.1403e-02],\n",
       "            ...,\n",
       "            [3.5639e-05, 4.8114e-04, 2.0011e-04,  ..., 2.6333e-03,\n",
       "             7.6857e-07, 1.8342e-04],\n",
       "            [7.0773e-05, 3.3936e-05, 2.3893e-03,  ..., 9.9529e-01,\n",
       "             3.6426e-04, 7.5117e-04],\n",
       "            [1.1288e-03, 1.6759e-04, 5.9475e-04,  ..., 1.0985e-04,\n",
       "             3.0630e-03, 4.9499e-01]],\n",
       "  \n",
       "           [[1.9597e-02, 7.0531e-03, 1.4895e-02,  ..., 1.3421e-02,\n",
       "             1.2941e-02, 4.3597e-01],\n",
       "            [2.9691e-03, 4.8075e-04, 4.3348e-05,  ..., 7.9539e-05,\n",
       "             7.9495e-05, 4.9700e-01],\n",
       "            [1.9769e-02, 1.4742e-02, 3.8694e-03,  ..., 1.4076e-04,\n",
       "             2.6691e-03, 4.6246e-01],\n",
       "            ...,\n",
       "            [1.1053e-02, 2.6772e-03, 3.1392e-03,  ..., 3.6610e-03,\n",
       "             4.5482e-03, 4.5955e-01],\n",
       "            [1.6748e-02, 1.5499e-03, 4.0992e-03,  ..., 5.2951e-02,\n",
       "             5.8299e-02, 4.2452e-01],\n",
       "            [4.5808e-02, 3.1279e-03, 1.0889e-03,  ..., 5.4068e-04,\n",
       "             3.0245e-03, 4.6695e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.1041e-02, 9.5689e-02, 9.7093e-02,  ..., 1.4966e-01,\n",
       "             7.8554e-02, 1.9551e-01],\n",
       "            [9.5939e-02, 6.3479e-03, 5.4321e-01,  ..., 1.6258e-02,\n",
       "             2.9263e-03, 1.6153e-01],\n",
       "            [2.3337e-04, 9.4903e-06, 6.3444e-04,  ..., 8.9565e-05,\n",
       "             1.0453e-03, 1.9911e-02],\n",
       "            ...,\n",
       "            [4.7278e-04, 7.6010e-07, 5.4119e-06,  ..., 9.8290e-04,\n",
       "             7.4846e-01, 1.2515e-01],\n",
       "            [1.0964e-05, 1.1689e-07, 3.1557e-08,  ..., 5.7954e-08,\n",
       "             1.4663e-04, 4.9885e-01],\n",
       "            [5.6487e-02, 6.2863e-03, 1.2322e-02,  ..., 1.9622e-03,\n",
       "             1.0904e-02, 4.4747e-01]],\n",
       "  \n",
       "           [[9.8455e-04, 2.2314e-01, 2.8325e-01,  ..., 6.3109e-02,\n",
       "             2.3823e-02, 9.2018e-02],\n",
       "            [1.9189e-03, 6.0714e-03, 1.7896e-02,  ..., 2.5887e-03,\n",
       "             2.1802e-03, 4.8049e-01],\n",
       "            [6.8247e-03, 9.5485e-04, 4.9953e-03,  ..., 3.8091e-03,\n",
       "             1.1842e-02, 2.8782e-01],\n",
       "            ...,\n",
       "            [4.4285e-03, 5.7704e-04, 1.0077e-03,  ..., 4.4985e-02,\n",
       "             2.6237e-01, 3.3403e-01],\n",
       "            [9.6207e-05, 2.5083e-03, 9.6113e-04,  ..., 2.3786e-04,\n",
       "             1.5073e-02, 4.8931e-01],\n",
       "            [6.7218e-03, 3.5762e-03, 5.0226e-03,  ..., 2.0993e-03,\n",
       "             6.3014e-03, 4.7977e-01]],\n",
       "  \n",
       "           [[2.2383e-01, 1.4699e-01, 5.3928e-02,  ..., 9.7970e-02,\n",
       "             2.1777e-02, 1.1861e-01],\n",
       "            [2.7750e-02, 3.1149e-02, 4.7994e-02,  ..., 2.5975e-02,\n",
       "             5.8558e-02, 2.7950e-01],\n",
       "            [1.8275e-02, 5.3510e-02, 1.7130e-02,  ..., 2.0260e-01,\n",
       "             9.0629e-02, 1.2269e-01],\n",
       "            ...,\n",
       "            [3.7148e-02, 4.2201e-02, 2.2979e-01,  ..., 3.6235e-02,\n",
       "             1.0973e-01, 1.9218e-01],\n",
       "            [5.4556e-02, 9.7311e-02, 2.2257e-02,  ..., 2.7451e-02,\n",
       "             3.1482e-02, 3.4155e-01],\n",
       "            [8.6631e-03, 6.6488e-03, 9.1988e-03,  ..., 6.1327e-03,\n",
       "             1.1073e-02, 4.6682e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[1.2056e-01, 2.1882e-01, 4.9757e-02,  ..., 1.3306e-01,\n",
       "             9.0341e-02, 7.2474e-02],\n",
       "            [2.5826e-03, 7.9952e-03, 7.2972e-04,  ..., 1.3321e-03,\n",
       "             8.3062e-04, 4.8714e-01],\n",
       "            [2.8280e-03, 8.8727e-01, 3.1463e-02,  ..., 6.6268e-03,\n",
       "             5.5104e-04, 2.4075e-02],\n",
       "            ...,\n",
       "            [7.6707e-05, 1.3948e-02, 1.1833e-03,  ..., 1.7479e-02,\n",
       "             1.0498e-03, 1.2129e-01],\n",
       "            [3.7834e-04, 1.0125e-02, 9.5019e-02,  ..., 7.9295e-01,\n",
       "             3.0235e-02, 2.6608e-02],\n",
       "            [1.6443e-03, 3.3824e-03, 2.0535e-03,  ..., 2.3690e-03,\n",
       "             8.1574e-03, 4.8516e-01]],\n",
       "  \n",
       "           [[2.9618e-03, 8.0778e-03, 4.4687e-03,  ..., 4.7365e-03,\n",
       "             1.1438e-04, 4.8099e-01],\n",
       "            [1.7312e-02, 7.9714e-03, 3.7217e-03,  ..., 3.3764e-03,\n",
       "             1.6268e-03, 4.7486e-01],\n",
       "            [4.3415e-03, 1.0335e-01, 4.9871e-02,  ..., 4.8074e-03,\n",
       "             1.1000e-03, 4.0761e-01],\n",
       "            ...,\n",
       "            [8.7941e-04, 1.4607e-03, 1.7002e-03,  ..., 1.8494e-03,\n",
       "             8.6914e-05, 3.1489e-01],\n",
       "            [9.6891e-04, 1.0015e-02, 1.7891e-02,  ..., 1.6666e-01,\n",
       "             1.4106e-03, 3.9248e-01],\n",
       "            [3.7346e-03, 3.9761e-03, 1.5395e-03,  ..., 7.6314e-04,\n",
       "             9.2718e-04, 4.9004e-01]],\n",
       "  \n",
       "           [[1.0537e-03, 2.8018e-04, 1.2100e-03,  ..., 1.6741e-03,\n",
       "             1.9538e-03, 4.9471e-01],\n",
       "            [1.6759e-03, 3.2825e-03, 1.4150e-02,  ..., 1.8600e-02,\n",
       "             1.1339e-02, 4.6151e-01],\n",
       "            [5.9594e-03, 3.8563e-03, 9.5244e-03,  ..., 3.5129e-03,\n",
       "             1.2157e-02, 4.4989e-01],\n",
       "            ...,\n",
       "            [5.2115e-03, 3.3602e-03, 3.3890e-03,  ..., 2.5446e-02,\n",
       "             2.1946e-02, 4.4970e-01],\n",
       "            [2.7492e-03, 2.2249e-03, 1.2342e-03,  ..., 4.9144e-03,\n",
       "             1.8197e-02, 4.7395e-01],\n",
       "            [9.4644e-03, 4.6856e-03, 3.8734e-03,  ..., 2.5995e-03,\n",
       "             8.0039e-03, 4.7813e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.8566e-02, 1.0753e-03, 4.8161e-03,  ..., 1.6323e-04,\n",
       "             4.5811e-03, 4.8314e-01],\n",
       "            [4.4406e-03, 2.8846e-03, 1.2400e-03,  ..., 1.8432e-04,\n",
       "             1.7673e-03, 4.9306e-01],\n",
       "            [1.1191e-03, 6.3313e-03, 3.7107e-03,  ..., 7.6676e-04,\n",
       "             1.8542e-03, 4.8666e-01],\n",
       "            ...,\n",
       "            [2.5237e-04, 1.2567e-03, 1.8329e-03,  ..., 7.4774e-03,\n",
       "             2.0557e-03, 4.7161e-01],\n",
       "            [2.4033e-03, 6.1593e-04, 1.0601e-02,  ..., 3.4017e-02,\n",
       "             7.7481e-01, 3.5073e-02],\n",
       "            [3.3118e-03, 1.5622e-03, 4.3489e-03,  ..., 1.1834e-03,\n",
       "             4.7881e-03, 4.8910e-01]],\n",
       "  \n",
       "           [[8.4490e-04, 2.9434e-03, 4.1724e-03,  ..., 1.9523e-03,\n",
       "             2.2990e-03, 4.9024e-01],\n",
       "            [2.9697e-03, 1.7690e-02, 4.0302e-02,  ..., 1.8111e-02,\n",
       "             1.2617e-02, 4.3227e-01],\n",
       "            [2.1287e-02, 8.5153e-02, 1.4391e-01,  ..., 3.1594e-02,\n",
       "             1.2581e-02, 2.6669e-01],\n",
       "            ...,\n",
       "            [3.8677e-03, 1.2835e-02, 2.5448e-02,  ..., 7.7029e-02,\n",
       "             4.1976e-03, 1.1466e-01],\n",
       "            [1.4162e-03, 4.3681e-02, 1.0784e-01,  ..., 3.8047e-01,\n",
       "             1.1697e-02, 1.4212e-01],\n",
       "            [5.5722e-03, 5.1507e-03, 4.5921e-03,  ..., 1.5639e-03,\n",
       "             2.9107e-03, 4.8489e-01]],\n",
       "  \n",
       "           [[4.7202e-03, 8.2500e-03, 7.3340e-03,  ..., 1.4407e-03,\n",
       "             7.6090e-03, 4.7865e-01],\n",
       "            [2.8355e-03, 1.4534e-03, 1.6400e-02,  ..., 3.0550e-03,\n",
       "             4.0542e-03, 4.8119e-01],\n",
       "            [8.2276e-03, 1.4202e-03, 3.2005e-03,  ..., 3.7706e-05,\n",
       "             3.5272e-03, 4.7338e-01],\n",
       "            ...,\n",
       "            [6.6606e-03, 7.9657e-03, 2.2867e-02,  ..., 6.1329e-03,\n",
       "             1.3495e-02, 3.0970e-01],\n",
       "            [1.2520e-03, 6.0004e-03, 3.9190e-03,  ..., 1.3364e-02,\n",
       "             6.0027e-03, 4.6638e-01],\n",
       "            [1.2493e-03, 1.0627e-03, 1.4648e-03,  ..., 2.5926e-03,\n",
       "             7.8062e-04, 4.9467e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[0.0419, 0.0038, 0.0573,  ..., 0.0313, 0.0176, 0.3995],\n",
       "            [0.0268, 0.0382, 0.0548,  ..., 0.0752, 0.3078, 0.1135],\n",
       "            [0.0388, 0.1013, 0.1502,  ..., 0.0300, 0.1907, 0.0630],\n",
       "            ...,\n",
       "            [0.0123, 0.0111, 0.0295,  ..., 0.0861, 0.0947, 0.0950],\n",
       "            [0.0024, 0.0100, 0.1318,  ..., 0.5089, 0.0228, 0.0961],\n",
       "            [0.0130, 0.0123, 0.0039,  ..., 0.0054, 0.0292, 0.4508]],\n",
       "  \n",
       "           [[0.2689, 0.0173, 0.2720,  ..., 0.0453, 0.0021, 0.1886],\n",
       "            [0.0022, 0.0335, 0.1633,  ..., 0.0492, 0.0336, 0.2950],\n",
       "            [0.0085, 0.0861, 0.0708,  ..., 0.0210, 0.0076, 0.3606],\n",
       "            ...,\n",
       "            [0.0015, 0.0546, 0.0158,  ..., 0.0673, 0.0081, 0.4114],\n",
       "            [0.0271, 0.1318, 0.0934,  ..., 0.2033, 0.0019, 0.2238],\n",
       "            [0.0052, 0.0064, 0.0047,  ..., 0.0025, 0.0016, 0.4860]],\n",
       "  \n",
       "           [[0.0126, 0.0514, 0.2234,  ..., 0.0746, 0.0210, 0.2663],\n",
       "            [0.0281, 0.0121, 0.0991,  ..., 0.0298, 0.0137, 0.3537],\n",
       "            [0.0980, 0.0766, 0.0342,  ..., 0.1516, 0.0514, 0.2370],\n",
       "            ...,\n",
       "            [0.0328, 0.0331, 0.0978,  ..., 0.0061, 0.0353, 0.3678],\n",
       "            [0.0240, 0.0127, 0.0268,  ..., 0.0120, 0.0112, 0.4357],\n",
       "            [0.0079, 0.0112, 0.0045,  ..., 0.0023, 0.0069, 0.4734]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[0.0968, 0.0145, 0.1616,  ..., 0.0703, 0.0146, 0.2944],\n",
       "            [0.0023, 0.0133, 0.6826,  ..., 0.1199, 0.0107, 0.0684],\n",
       "            [0.0030, 0.0028, 0.0370,  ..., 0.0139, 0.0487, 0.1522],\n",
       "            ...,\n",
       "            [0.0032, 0.0059, 0.0078,  ..., 0.1225, 0.3096, 0.2643],\n",
       "            [0.0103, 0.0748, 0.2339,  ..., 0.1627, 0.0294, 0.2259],\n",
       "            [0.1038, 0.0116, 0.0319,  ..., 0.0109, 0.0030, 0.4136]],\n",
       "  \n",
       "           [[0.2225, 0.0042, 0.0046,  ..., 0.0035, 0.0598, 0.3377],\n",
       "            [0.0048, 0.0106, 0.0999,  ..., 0.0376, 0.0599, 0.2649],\n",
       "            [0.0168, 0.0570, 0.0266,  ..., 0.0452, 0.2854, 0.1117],\n",
       "            ...,\n",
       "            [0.0041, 0.0656, 0.0343,  ..., 0.1663, 0.3858, 0.0982],\n",
       "            [0.0092, 0.0442, 0.0360,  ..., 0.1019, 0.0444, 0.3516],\n",
       "            [0.0123, 0.0095, 0.0204,  ..., 0.0119, 0.0042, 0.4575]],\n",
       "  \n",
       "           [[0.1279, 0.0459, 0.2144,  ..., 0.1365, 0.0079, 0.2122],\n",
       "            [0.0090, 0.2388, 0.0105,  ..., 0.0065, 0.0063, 0.3388],\n",
       "            [0.0061, 0.0163, 0.7627,  ..., 0.0178, 0.0051, 0.0750],\n",
       "            ...,\n",
       "            [0.0079, 0.0287, 0.0133,  ..., 0.1511, 0.0058, 0.3869],\n",
       "            [0.0046, 0.0670, 0.0096,  ..., 0.0547, 0.0280, 0.3398],\n",
       "            [0.0048, 0.0114, 0.0075,  ..., 0.0076, 0.0044, 0.4765]]]],\n",
       "         grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[5.2835e-03, 3.4991e-02, 2.8248e-01,  ..., 1.3977e-01,\n",
       "             1.6870e-01, 5.5732e-02],\n",
       "            [3.3831e-03, 1.7356e-02, 4.0739e-01,  ..., 5.1065e-02,\n",
       "             2.9115e-02, 1.7444e-01],\n",
       "            [1.2979e-02, 6.2010e-03, 3.0248e-02,  ..., 2.8753e-03,\n",
       "             4.4923e-03, 4.5283e-01],\n",
       "            ...,\n",
       "            [1.7026e-03, 8.2814e-03, 3.6351e-02,  ..., 2.9243e-02,\n",
       "             1.0122e-01, 3.9657e-01],\n",
       "            [3.7498e-03, 1.3161e-02, 6.7812e-02,  ..., 4.9513e-02,\n",
       "             3.4234e-03, 4.2073e-01],\n",
       "            [1.7533e-02, 5.5791e-03, 1.9661e-02,  ..., 1.3754e-02,\n",
       "             1.3584e-03, 4.6460e-01]],\n",
       "  \n",
       "           [[3.4056e-01, 2.2376e-03, 5.6262e-02,  ..., 1.4240e-02,\n",
       "             5.0689e-03, 2.8557e-01],\n",
       "            [2.8785e-04, 4.7863e-02, 4.0360e-04,  ..., 1.2561e-04,\n",
       "             3.0719e-04, 4.7507e-01],\n",
       "            [8.4228e-04, 1.0342e-04, 1.8262e-01,  ..., 7.5033e-05,\n",
       "             1.5937e-04, 4.0792e-01],\n",
       "            ...,\n",
       "            [6.4839e-05, 3.0101e-05, 9.5266e-05,  ..., 8.8294e-03,\n",
       "             3.6405e-04, 4.9519e-01],\n",
       "            [2.7548e-04, 2.5484e-03, 1.3458e-03,  ..., 2.7307e-03,\n",
       "             7.4718e-03, 4.8437e-01],\n",
       "            [3.4792e-03, 5.8053e-04, 2.6937e-03,  ..., 6.7915e-04,\n",
       "             7.5276e-04, 4.9495e-01]],\n",
       "  \n",
       "           [[6.4893e-02, 3.3294e-02, 3.3605e-01,  ..., 1.1527e-01,\n",
       "             1.3387e-01, 4.6397e-02],\n",
       "            [6.8781e-03, 8.2713e-03, 7.6532e-03,  ..., 2.1184e-02,\n",
       "             3.7726e-03, 4.6617e-01],\n",
       "            [1.1252e-01, 1.1565e-01, 6.4609e-02,  ..., 8.7765e-03,\n",
       "             7.4315e-03, 3.2223e-01],\n",
       "            ...,\n",
       "            [4.5113e-02, 1.2291e-02, 4.2901e-02,  ..., 2.0272e-02,\n",
       "             1.5575e-02, 4.0654e-01],\n",
       "            [8.6535e-02, 1.1739e-01, 1.5259e-01,  ..., 6.1068e-02,\n",
       "             6.7780e-03, 2.6220e-01],\n",
       "            [3.5079e-02, 1.2071e-02, 1.0585e-02,  ..., 3.2816e-02,\n",
       "             1.2958e-02, 4.2569e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.7885e-01, 5.2779e-03, 5.5998e-01,  ..., 3.0059e-02,\n",
       "             1.1244e-03, 1.0694e-01],\n",
       "            [9.8649e-04, 9.0565e-02, 2.6080e-01,  ..., 4.2148e-02,\n",
       "             3.4597e-03, 2.8435e-01],\n",
       "            [1.1736e-03, 5.9584e-02, 8.0278e-01,  ..., 4.4280e-02,\n",
       "             6.4063e-04, 3.5376e-02],\n",
       "            ...,\n",
       "            [1.9626e-03, 4.3405e-02, 4.5537e-01,  ..., 1.0405e-01,\n",
       "             2.0631e-03, 1.7810e-01],\n",
       "            [1.4774e-03, 6.7905e-02, 3.9100e-01,  ..., 2.2304e-01,\n",
       "             5.7137e-03, 1.2281e-01],\n",
       "            [2.3311e-03, 6.5933e-03, 4.5090e-03,  ..., 2.8050e-03,\n",
       "             1.8444e-03, 4.8827e-01]],\n",
       "  \n",
       "           [[6.2018e-02, 6.1522e-02, 1.6728e-01,  ..., 1.7011e-01,\n",
       "             6.1563e-02, 1.6781e-01],\n",
       "            [9.7046e-04, 9.4452e-03, 2.2581e-02,  ..., 7.2947e-03,\n",
       "             8.0952e-03, 4.6330e-01],\n",
       "            [3.5918e-03, 1.4573e-02, 3.4424e-02,  ..., 7.0448e-03,\n",
       "             3.4765e-03, 4.5739e-01],\n",
       "            ...,\n",
       "            [4.3205e-03, 1.7278e-02, 9.2389e-03,  ..., 1.0419e-02,\n",
       "             5.0183e-03, 4.6690e-01],\n",
       "            [1.1677e-03, 4.4966e-02, 6.9435e-02,  ..., 1.5368e-02,\n",
       "             7.8128e-03, 3.8908e-01],\n",
       "            [4.2344e-03, 8.5576e-03, 9.5719e-03,  ..., 9.7192e-03,\n",
       "             1.1310e-02, 4.6180e-01]],\n",
       "  \n",
       "           [[4.5024e-03, 2.4331e-02, 2.7346e-01,  ..., 6.2311e-02,\n",
       "             1.3916e-01, 1.4140e-01],\n",
       "            [1.5718e-03, 2.7268e-03, 1.2829e-01,  ..., 6.3521e-03,\n",
       "             4.3137e-04, 4.2746e-01],\n",
       "            [6.7301e-03, 4.7767e-03, 1.8026e-02,  ..., 9.4008e-04,\n",
       "             7.3288e-04, 4.7325e-01],\n",
       "            ...,\n",
       "            [8.5842e-04, 1.7564e-03, 3.3111e-03,  ..., 1.1680e-02,\n",
       "             5.3007e-03, 4.8554e-01],\n",
       "            [7.2530e-03, 2.8944e-01, 4.4640e-02,  ..., 2.7748e-01,\n",
       "             8.1527e-03, 6.4147e-02],\n",
       "            [2.4452e-02, 4.6927e-02, 2.5567e-02,  ..., 1.7161e-02,\n",
       "             2.1991e-02, 4.0676e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[2.5888e-05, 3.7906e-04, 5.0890e-04,  ..., 3.5718e-04,\n",
       "             2.1970e-04, 4.9901e-01],\n",
       "            [2.0705e-05, 1.0411e-04, 1.6685e-04,  ..., 5.1119e-04,\n",
       "             8.7422e-05, 4.9942e-01],\n",
       "            [4.2100e-05, 5.2221e-04, 8.0370e-05,  ..., 4.3894e-04,\n",
       "             1.7856e-04, 4.9911e-01],\n",
       "            ...,\n",
       "            [1.0180e-05, 1.6416e-04, 1.6968e-04,  ..., 1.4479e-04,\n",
       "             1.3203e-04, 4.9950e-01],\n",
       "            [5.3544e-05, 7.2755e-04, 4.7319e-04,  ..., 2.5241e-03,\n",
       "             1.0214e-03, 4.9681e-01],\n",
       "            [9.6517e-03, 1.8704e-02, 1.0408e-02,  ..., 1.3076e-02,\n",
       "             4.2718e-03, 4.5775e-01]],\n",
       "  \n",
       "           [[3.0150e-01, 1.2059e-02, 1.6623e-01,  ..., 3.0970e-02,\n",
       "             4.0789e-02, 1.8198e-01],\n",
       "            [7.0280e-02, 2.9558e-02, 9.3918e-02,  ..., 5.6096e-02,\n",
       "             1.7446e-02, 3.3579e-01],\n",
       "            [4.7766e-01, 3.1240e-03, 2.4517e-01,  ..., 4.3729e-03,\n",
       "             8.8151e-03, 1.1098e-01],\n",
       "            ...,\n",
       "            [7.5645e-02, 5.0313e-03, 6.8413e-03,  ..., 4.4050e-03,\n",
       "             7.2961e-03, 4.4153e-01],\n",
       "            [4.5781e-02, 1.4267e-02, 2.1300e-02,  ..., 1.7551e-02,\n",
       "             1.7556e-02, 4.1403e-01],\n",
       "            [1.5854e-02, 1.9914e-02, 1.6649e-02,  ..., 1.1556e-02,\n",
       "             7.4503e-03, 4.4736e-01]],\n",
       "  \n",
       "           [[1.8301e-02, 6.3728e-02, 7.6565e-01,  ..., 1.2265e-01,\n",
       "             1.1735e-03, 8.6460e-03],\n",
       "            [2.6905e-02, 7.5970e-02, 3.2240e-01,  ..., 1.1968e-01,\n",
       "             4.9603e-03, 2.1871e-01],\n",
       "            [3.1941e-02, 1.7898e-02, 7.0390e-01,  ..., 5.1569e-02,\n",
       "             8.7646e-03, 6.2354e-02],\n",
       "            ...,\n",
       "            [1.9020e-02, 2.3225e-02, 1.7168e-01,  ..., 2.4379e-01,\n",
       "             5.4172e-03, 2.6075e-01],\n",
       "            [8.4929e-03, 6.2706e-02, 1.2587e-01,  ..., 3.0759e-01,\n",
       "             7.3982e-03, 2.1872e-01],\n",
       "            [7.7413e-03, 4.9702e-03, 3.7966e-03,  ..., 5.6271e-03,\n",
       "             1.3495e-02, 4.6770e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[1.1605e-02, 2.0077e-02, 5.7339e-01,  ..., 5.6471e-02,\n",
       "             9.2562e-02, 2.7877e-02],\n",
       "            [1.7097e-02, 1.8310e-02, 1.9055e-01,  ..., 7.4181e-02,\n",
       "             7.9904e-02, 2.0598e-01],\n",
       "            [8.2833e-03, 1.4100e-02, 7.7411e-02,  ..., 1.1543e-02,\n",
       "             3.8361e-02, 3.8689e-01],\n",
       "            ...,\n",
       "            [1.1857e-03, 6.2243e-03, 1.3371e-02,  ..., 1.2850e-02,\n",
       "             4.4234e-02, 4.3475e-01],\n",
       "            [2.4802e-03, 4.3280e-02, 6.9687e-02,  ..., 1.9206e-01,\n",
       "             1.4494e-02, 3.1111e-01],\n",
       "            [7.4972e-03, 9.6583e-03, 1.2994e-02,  ..., 1.8172e-02,\n",
       "             7.2376e-03, 4.5706e-01]],\n",
       "  \n",
       "           [[3.4473e-03, 9.4283e-04, 1.6578e-03,  ..., 6.4714e-03,\n",
       "             4.0836e-03, 4.8737e-01],\n",
       "            [1.3159e-03, 1.6908e-03, 6.0223e-04,  ..., 4.5053e-03,\n",
       "             7.0264e-03, 4.8619e-01],\n",
       "            [1.3492e-03, 1.7838e-03, 6.2912e-04,  ..., 8.5794e-03,\n",
       "             6.0720e-03, 4.8682e-01],\n",
       "            ...,\n",
       "            [3.2959e-03, 3.2626e-03, 7.4303e-03,  ..., 2.6177e-03,\n",
       "             1.1978e-02, 4.7517e-01],\n",
       "            [4.0070e-03, 5.8930e-03, 7.5763e-03,  ..., 3.2729e-02,\n",
       "             1.9436e-01, 3.0378e-01],\n",
       "            [8.4693e-02, 3.1908e-02, 5.7409e-02,  ..., 1.2520e-01,\n",
       "             3.0166e-01, 5.6537e-02]],\n",
       "  \n",
       "           [[1.3317e-03, 2.2706e-03, 1.0067e-03,  ..., 1.0525e-03,\n",
       "             1.1745e-03, 4.9270e-01],\n",
       "            [2.3244e-02, 1.1819e-02, 6.6835e-03,  ..., 1.4072e-03,\n",
       "             5.7551e-03, 4.6163e-01],\n",
       "            [1.7521e-02, 1.2728e-02, 4.0008e-03,  ..., 1.0804e-03,\n",
       "             3.5391e-03, 4.6805e-01],\n",
       "            ...,\n",
       "            [1.0372e-02, 1.3241e-02, 7.4899e-03,  ..., 2.7047e-03,\n",
       "             4.3389e-03, 4.6375e-01],\n",
       "            [1.6988e-01, 1.9760e-01, 7.5148e-02,  ..., 1.2488e-02,\n",
       "             3.2490e-02, 1.4235e-01],\n",
       "            [1.1664e-02, 4.4063e-03, 1.1479e-03,  ..., 2.0821e-03,\n",
       "             1.0749e-03, 4.8488e-01]]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[5.7760e-03, 2.0458e-02, 1.4435e-02,  ..., 1.7283e-02,\n",
       "             1.7898e-02, 4.4435e-01],\n",
       "            [4.8582e-04, 1.4376e-02, 5.4424e-03,  ..., 1.5130e-02,\n",
       "             8.8440e-03, 4.6867e-01],\n",
       "            [1.6153e-03, 1.4907e-02, 7.4117e-03,  ..., 7.3272e-03,\n",
       "             7.1098e-03, 4.7203e-01],\n",
       "            ...,\n",
       "            [1.9880e-03, 2.0365e-02, 5.3736e-03,  ..., 1.2579e-02,\n",
       "             9.0172e-03, 4.6138e-01],\n",
       "            [6.8110e-04, 9.7167e-03, 5.3391e-03,  ..., 1.0896e-02,\n",
       "             8.5593e-03, 4.7573e-01],\n",
       "            [7.7821e-02, 9.9282e-02, 8.9445e-02,  ..., 9.7269e-02,\n",
       "             1.1402e-01, 1.0417e-01]],\n",
       "  \n",
       "           [[7.9514e-03, 1.1192e-01, 6.9866e-01,  ..., 9.3546e-02,\n",
       "             1.0869e-03, 8.5473e-03],\n",
       "            [3.0062e-03, 2.0035e-01, 2.8750e-01,  ..., 1.2491e-01,\n",
       "             7.4438e-03, 1.1755e-01],\n",
       "            [1.3822e-03, 1.0294e-01, 6.1119e-01,  ..., 1.7881e-01,\n",
       "             1.5163e-03, 2.4407e-02],\n",
       "            ...,\n",
       "            [3.2125e-03, 1.1959e-01, 2.3173e-01,  ..., 2.3980e-01,\n",
       "             8.2130e-03, 1.2568e-01],\n",
       "            [2.4240e-03, 9.4806e-02, 1.3325e-01,  ..., 1.9544e-01,\n",
       "             4.7495e-03, 2.2624e-01],\n",
       "            [7.1895e-02, 9.9815e-02, 8.9576e-02,  ..., 7.3683e-02,\n",
       "             1.0174e-01, 1.5218e-01]],\n",
       "  \n",
       "           [[1.5496e-03, 8.7194e-03, 1.8699e-02,  ..., 5.3370e-03,\n",
       "             1.2916e-04, 4.8070e-01],\n",
       "            [1.4070e-05, 3.3598e-03, 5.0685e-03,  ..., 4.4010e-04,\n",
       "             3.4977e-05, 4.9516e-01],\n",
       "            [5.0059e-05, 3.8568e-03, 8.5637e-01,  ..., 3.1014e-02,\n",
       "             6.3163e-05, 5.3287e-02],\n",
       "            ...,\n",
       "            [8.1527e-06, 3.0414e-03, 5.7630e-02,  ..., 1.3775e-02,\n",
       "             7.2469e-05, 4.6154e-01],\n",
       "            [2.5705e-05, 4.8336e-03, 2.6122e-02,  ..., 5.7591e-03,\n",
       "             3.2982e-04, 4.7945e-01],\n",
       "            [2.7628e-02, 5.3734e-02, 4.2568e-02,  ..., 3.4409e-02,\n",
       "             3.3308e-02, 3.3587e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[2.1432e-02, 1.7865e-02, 2.1127e-02,  ..., 1.5266e-02,\n",
       "             4.0870e-02, 3.6110e-01],\n",
       "            [7.0212e-03, 6.2158e-02, 2.1392e-02,  ..., 1.9141e-02,\n",
       "             3.9096e-02, 3.7987e-01],\n",
       "            [1.2724e-03, 2.8022e-02, 8.8678e-03,  ..., 7.2603e-03,\n",
       "             7.6949e-03, 4.5661e-01],\n",
       "            ...,\n",
       "            [1.3077e-03, 1.3870e-02, 6.6764e-03,  ..., 2.4704e-02,\n",
       "             3.3894e-02, 4.4200e-01],\n",
       "            [1.1295e-02, 7.4729e-02, 3.8494e-02,  ..., 1.2397e-02,\n",
       "             1.8108e-02, 3.7592e-01],\n",
       "            [1.5805e-01, 9.5684e-02, 1.1535e-01,  ..., 9.5877e-02,\n",
       "             7.7113e-02, 7.7721e-02]],\n",
       "  \n",
       "           [[4.8848e-03, 6.7580e-02, 6.7012e-01,  ..., 7.0239e-02,\n",
       "             4.7989e-03, 4.2724e-02],\n",
       "            [2.2960e-03, 1.2885e-01, 2.5423e-01,  ..., 6.4179e-02,\n",
       "             1.0666e-02, 2.2699e-01],\n",
       "            [1.8128e-03, 1.7705e-01, 4.6969e-01,  ..., 2.3804e-02,\n",
       "             1.7730e-03, 1.1138e-01],\n",
       "            ...,\n",
       "            [3.9026e-03, 1.4035e-01, 1.1333e-01,  ..., 5.8440e-02,\n",
       "             8.7927e-03, 2.8300e-01],\n",
       "            [2.0949e-03, 7.6366e-02, 7.6102e-02,  ..., 2.8617e-02,\n",
       "             2.8836e-03, 3.7047e-01],\n",
       "            [8.8990e-02, 1.0984e-01, 1.0584e-01,  ..., 9.0254e-02,\n",
       "             7.2293e-02, 1.3499e-01]],\n",
       "  \n",
       "           [[1.2743e-02, 8.0190e-02, 2.6582e-01,  ..., 4.2554e-02,\n",
       "             1.2301e-02, 2.6816e-01],\n",
       "            [3.2780e-05, 1.3028e-02, 3.9815e-03,  ..., 4.2549e-04,\n",
       "             3.2055e-05, 4.8891e-01],\n",
       "            [9.3114e-05, 1.0361e-02, 4.5820e-01,  ..., 2.5966e-02,\n",
       "             3.1441e-04, 2.5069e-01],\n",
       "            ...,\n",
       "            [1.7875e-06, 1.8984e-03, 1.4479e-02,  ..., 1.9367e-02,\n",
       "             9.7142e-06, 4.8111e-01],\n",
       "            [4.4864e-04, 6.7298e-02, 3.0523e-02,  ..., 2.3844e-02,\n",
       "             4.2135e-03, 4.0254e-01],\n",
       "            [5.3221e-02, 7.9292e-02, 6.2407e-02,  ..., 7.9652e-02,\n",
       "             6.9836e-02, 2.2257e-01]]]], grad_fn=<SoftmaxBackward>)))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
